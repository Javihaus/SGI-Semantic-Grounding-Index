{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2ENdi16fdM1"
      },
      "source": [
        "# Context-Anchored Trajectory Analysis: HaluEval Validation\n",
        "\n",
        "**Key Insight from TruthfulQA Failure:**\n",
        "\n",
        "TruthfulQA tests *misconceptions* - plausible-sounding false beliefs. These are semantically CLOSE to the question because they're the naive, expected response.\n",
        "\n",
        "**Refined Hypothesis:**\n",
        "\n",
        "For RAG/grounded generation, measure distance from **CONTEXT**, not question:\n",
        "\n",
        "- Fabrications: claims not entailed by context → launch far from context embedding\n",
        "- Valid responses: paraphrase/summarize context → stay close to context embedding\n",
        "\n",
        "HaluEval provides (question, knowledge/context, answer, hallucination_label) - exactly what we need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAcQ4hL0fdM2"
      },
      "outputs": [],
      "source": [
        "#!pip install -q datasets sentence-transformers spacy numpy pandas matplotlib seaborn scipy scikit-learn\n",
        "#!python -m spacy download en_core_web_sm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bhu4ApSfdM3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, List, Dict\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import load_dataset\n",
        "from scipy import stats\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "print(\"Libraries loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ofuuaPRfdM3"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"all-mpnet-base-v2\"\n",
        "print(f\"Loading embedding model: {MODEL_NAME}\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "encoder = SentenceTransformer(MODEL_NAME)\n",
        "print(f\"Embedding dimension: {encoder.get_sentence_embedding_dimension()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0VuZSntfdM3"
      },
      "source": [
        "## 1. Load HaluEval Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HJVLoL9fdM3"
      },
      "outputs": [],
      "source": [
        "print(\"Loading HaluEval QA dataset...\")\n",
        "dataset = load_dataset(\"pminervini/HaluEval\", \"qa_samples\", split=\"data\")\n",
        "\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(f\"Columns: {dataset.column_names}\")\n",
        "\n",
        "# Check hallucination field values\n",
        "halluc_values = {}\n",
        "for item in dataset:\n",
        "    val = str(item.get(\"hallucination\", \"MISSING\"))\n",
        "    halluc_values[val] = halluc_values.get(val, 0) + 1\n",
        "print(f\"\\nHallucination field distribution: {halluc_values}\")\n",
        "\n",
        "# Show example\n",
        "print(\"\\n=== EXAMPLE ===\")\n",
        "ex = dataset[0]\n",
        "for k, v in ex.items():\n",
        "    print(f\"{k}: {str(v)[:100]}...\" if len(str(v)) > 100 else f\"{k}: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xtv-rsojfdM4"
      },
      "outputs": [],
      "source": [
        "def prepare_halueval_cases(dataset, max_samples: int = 500) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Convert HaluEval to test cases.\n",
        "    Handles yes/no, 0/1, true/false formats for hallucination field.\n",
        "    \"\"\"\n",
        "    cases = []\n",
        "\n",
        "    for i, item in enumerate(dataset):\n",
        "        if len(cases) >= max_samples:\n",
        "            break\n",
        "\n",
        "        if not item.get(\"knowledge\") or not item.get(\"answer\"):\n",
        "            continue\n",
        "\n",
        "        if len(item[\"answer\"]) < 20:\n",
        "            continue\n",
        "\n",
        "        # Parse hallucination field - handles multiple formats\n",
        "        halluc_val = str(item.get(\"hallucination\", \"\")).lower().strip()\n",
        "        is_hallucinated = halluc_val in [\"yes\", \"1\", \"true\"]\n",
        "        is_valid_answer = halluc_val in [\"no\", \"0\", \"false\"]\n",
        "\n",
        "        if not is_hallucinated and not is_valid_answer:\n",
        "            continue\n",
        "\n",
        "        cases.append({\n",
        "            \"id\": f\"halueval_{i}\",\n",
        "            \"question\": item[\"question\"],\n",
        "            \"context\": item[\"knowledge\"],\n",
        "            \"output\": item[\"answer\"],\n",
        "            \"is_valid\": is_valid_answer,\n",
        "            \"source\": \"halueval\"\n",
        "        })\n",
        "\n",
        "    return cases\n",
        "\n",
        "MAX_SAMPLES = 1000\n",
        "ALL_CASES = prepare_halueval_cases(dataset, max_samples=MAX_SAMPLES)\n",
        "\n",
        "print(f\"\\nPrepared {len(ALL_CASES)} test cases:\")\n",
        "print(f\"  Valid (grounded): {sum(1 for c in ALL_CASES if c['is_valid'])}\")\n",
        "print(f\"  Hallucinated: {sum(1 for c in ALL_CASES if not c['is_valid'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEYY4dlRfdM4"
      },
      "outputs": [],
      "source": [
        "# Examine examples\n",
        "valid_cases = [c for c in ALL_CASES if c['is_valid']]\n",
        "halluc_cases = [c for c in ALL_CASES if not c['is_valid']]\n",
        "\n",
        "if valid_cases:\n",
        "    print(\"=== VALID (GROUNDED) EXAMPLE ===\")\n",
        "    ex = valid_cases[0]\n",
        "    print(f\"Question: {ex['question'][:100]}...\")\n",
        "    print(f\"Context: {ex['context'][:150]}...\")\n",
        "    print(f\"Answer: {ex['output'][:150]}...\")\n",
        "else:\n",
        "    print(\"WARNING: No valid cases found!\")\n",
        "\n",
        "if halluc_cases:\n",
        "    print(\"\\n=== HALLUCINATED EXAMPLE ===\")\n",
        "    ex = halluc_cases[0]\n",
        "    print(f\"Question: {ex['question'][:100]}...\")\n",
        "    print(f\"Context: {ex['context'][:150]}...\")\n",
        "    print(f\"Answer: {ex['output'][:150]}...\")\n",
        "else:\n",
        "    print(\"WARNING: No hallucinated cases found!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3suN2HeofdM4"
      },
      "source": [
        "## 2. Context-Anchored Trajectory Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbg4XuX-fdM4"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ContextAnchoredMetrics:\n",
        "    launch_from_context: float\n",
        "    mean_distance_from_context: float\n",
        "    max_distance_from_context: float\n",
        "    final_distance_from_context: float\n",
        "    launch_from_question: float\n",
        "    mean_distance_from_question: float\n",
        "    grounding_ratio: float\n",
        "    context_question_alignment: float\n",
        "    path_length: float\n",
        "    efficiency: float\n",
        "    return_to_context_ratio: float\n",
        "    mean_angular_deviation: float\n",
        "    num_claims: int\n",
        "    context_length: int\n",
        "\n",
        "    def to_dict(self) -> dict:\n",
        "        return self.__dict__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KRxvzrzfdM4"
      },
      "outputs": [],
      "source": [
        "def extract_claims(text: str) -> List[str]:\n",
        "    doc = nlp(text)\n",
        "    return [s.text.strip() for s in doc.sents if len(s.text.strip()) > 5]\n",
        "\n",
        "\n",
        "def compute_context_anchored_metrics(question: str, context: str, output: str) -> Optional[ContextAnchoredMetrics]:\n",
        "    claims = extract_claims(output)\n",
        "    if len(claims) == 0:\n",
        "        return None\n",
        "\n",
        "    if len(claims) == 1:\n",
        "        claims = [claims[0], claims[0]]\n",
        "\n",
        "    ctx_emb = encoder.encode(context)\n",
        "    q_emb = encoder.encode(question)\n",
        "    claim_embeddings = [encoder.encode(c) for c in claims]\n",
        "\n",
        "    trajectory = [ctx_emb] + claim_embeddings\n",
        "    momenta = [trajectory[i+1] - trajectory[i] for i in range(len(trajectory)-1)]\n",
        "\n",
        "    launch_from_context = np.linalg.norm(momenta[0])\n",
        "    distances_from_ctx = [np.linalg.norm(emb - ctx_emb) for emb in claim_embeddings]\n",
        "\n",
        "    launch_from_question = np.linalg.norm(claim_embeddings[0] - q_emb)\n",
        "    distances_from_q = [np.linalg.norm(emb - q_emb) for emb in claim_embeddings]\n",
        "\n",
        "    grounding_ratio = launch_from_context / (launch_from_question + 1e-8)\n",
        "\n",
        "    ctx_to_q = q_emb - ctx_emb\n",
        "    ctx_to_claim = momenta[0]\n",
        "    norm_product = np.linalg.norm(ctx_to_q) * np.linalg.norm(ctx_to_claim)\n",
        "    context_question_alignment = np.dot(ctx_to_q, ctx_to_claim) / norm_product if norm_product > 1e-8 else 0.0\n",
        "\n",
        "    path_length = sum(np.linalg.norm(m) for m in momenta)\n",
        "    direct_distance = np.linalg.norm(trajectory[-1] - trajectory[0])\n",
        "    efficiency = direct_distance / (path_length + 1e-8)\n",
        "    return_to_context_ratio = distances_from_ctx[-1] / (max(distances_from_ctx) + 1e-8)\n",
        "\n",
        "    angles = []\n",
        "    for i in range(len(momenta) - 1):\n",
        "        norm_product = np.linalg.norm(momenta[i]) * np.linalg.norm(momenta[i+1])\n",
        "        if norm_product > 1e-8:\n",
        "            cos_angle = np.dot(momenta[i], momenta[i+1]) / norm_product\n",
        "            angles.append(np.arccos(np.clip(cos_angle, -1, 1)))\n",
        "\n",
        "    return ContextAnchoredMetrics(\n",
        "        launch_from_context=launch_from_context,\n",
        "        mean_distance_from_context=np.mean(distances_from_ctx),\n",
        "        max_distance_from_context=np.max(distances_from_ctx),\n",
        "        final_distance_from_context=distances_from_ctx[-1],\n",
        "        launch_from_question=launch_from_question,\n",
        "        mean_distance_from_question=np.mean(distances_from_q),\n",
        "        grounding_ratio=grounding_ratio,\n",
        "        context_question_alignment=context_question_alignment,\n",
        "        path_length=path_length,\n",
        "        efficiency=efficiency,\n",
        "        return_to_context_ratio=return_to_context_ratio,\n",
        "        mean_angular_deviation=np.mean(angles) if angles else 0.0,\n",
        "        num_claims=len(claims),\n",
        "        context_length=len(context)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xab4R7ecfdM5"
      },
      "source": [
        "## 3. Run Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFZU_KSffdM5"
      },
      "outputs": [],
      "source": [
        "def analyze_all_cases(cases: List[Dict]) -> pd.DataFrame:\n",
        "    results = []\n",
        "    failed = 0\n",
        "\n",
        "    for case in tqdm(cases, desc=\"Analyzing trajectories\"):\n",
        "        try:\n",
        "            metrics = compute_context_anchored_metrics(\n",
        "                question=case[\"question\"],\n",
        "                context=case[\"context\"],\n",
        "                output=case[\"output\"]\n",
        "            )\n",
        "            if metrics is None:\n",
        "                failed += 1\n",
        "                continue\n",
        "\n",
        "            result = metrics.to_dict()\n",
        "            result[\"id\"] = case[\"id\"]\n",
        "            result[\"is_valid\"] = case[\"is_valid\"]\n",
        "            results.append(result)\n",
        "        except:\n",
        "            failed += 1\n",
        "\n",
        "    print(f\"\\nProcessed {len(results)} cases, {failed} failed.\")\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "print(f\"Analyzing {len(ALL_CASES)} cases...\")\n",
        "df = analyze_all_cases(ALL_CASES)\n",
        "\n",
        "print(f\"\\nFinal: {len(df)} total, {len(df[df['is_valid']])} valid, {len(df[~df['is_valid']])} hallucinated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HGFKBREfdM5"
      },
      "source": [
        "## 4. Statistical Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2a59zz2fdM5"
      },
      "outputs": [],
      "source": [
        "METRICS = [\"launch_from_context\", \"mean_distance_from_context\", \"max_distance_from_context\",\n",
        "           \"final_distance_from_context\", \"launch_from_question\", \"mean_distance_from_question\",\n",
        "           \"grounding_ratio\", \"context_question_alignment\", \"efficiency\",\n",
        "           \"return_to_context_ratio\", \"mean_angular_deviation\"]\n",
        "\n",
        "EXPECTED = {\n",
        "    \"launch_from_context\": \"higher\",\n",
        "    \"mean_distance_from_context\": \"higher\",\n",
        "    \"max_distance_from_context\": \"higher\",\n",
        "    \"final_distance_from_context\": \"higher\",\n",
        "    \"launch_from_question\": \"lower\",\n",
        "    \"mean_distance_from_question\": \"lower\",\n",
        "    \"grounding_ratio\": \"higher\",\n",
        "    \"context_question_alignment\": \"lower\",\n",
        "    \"efficiency\": \"lower\",\n",
        "    \"return_to_context_ratio\": \"higher\",\n",
        "    \"mean_angular_deviation\": \"higher\"\n",
        "}\n",
        "\n",
        "def compute_stats(df, metric):\n",
        "    valid = df[df[\"is_valid\"]][metric].dropna()\n",
        "    halluc = df[~df[\"is_valid\"]][metric].dropna()\n",
        "\n",
        "    diff = halluc.mean() - valid.mean()\n",
        "    pooled_std = np.sqrt((valid.std()**2 + halluc.std()**2) / 2)\n",
        "    cohens_d = diff / pooled_std if pooled_std > 1e-8 else 0\n",
        "    _, p_value = stats.ttest_ind(halluc, valid, equal_var=False)\n",
        "\n",
        "    expected = EXPECTED.get(metric)\n",
        "    confirmed = (diff > 0 and p_value < 0.05) if expected == \"higher\" else (diff < 0 and p_value < 0.05) if expected == \"lower\" else None\n",
        "\n",
        "    return {\"metric\": metric, \"valid_mean\": valid.mean(), \"halluc_mean\": halluc.mean(),\n",
        "            \"diff\": diff, \"cohens_d\": cohens_d, \"p_value\": p_value,\n",
        "            \"significant\": p_value < 0.05, \"confirmed\": confirmed, \"expected\": expected}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNhDDppAfdM5"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 100)\n",
        "print(\"CONTEXT-ANCHORED TRAJECTORY ANALYSIS: HaluEval\")\n",
        "print(\"=\" * 100)\n",
        "print(f\"\\nSamples: {len(df)} ({len(df[df['is_valid']])} valid, {len(df[~df['is_valid']])} hallucinated)\")\n",
        "print(f\"\\nKey Hypothesis: Fabricated claims launch FURTHER from context than grounded claims.\\n\")\n",
        "\n",
        "stats_results = [compute_stats(df, m) for m in METRICS if m in df.columns]\n",
        "\n",
        "print(\"-\" * 100)\n",
        "print(f\"{'Metric':<35} | {'Valid':>8} | {'Halluc':>8} | {'Diff':>8} | {'d':>7} | {'p-value':>10} | Hyp\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for r in stats_results:\n",
        "    hyp = \"✓\" if r[\"confirmed\"] else \"✗\" if r[\"confirmed\"] is False else \"?\"\n",
        "    sig = \"*\" if r[\"significant\"] else \"\"\n",
        "    print(f\"{r['metric']:<35} | {r['valid_mean']:>8.4f} | {r['halluc_mean']:>8.4f} | \"\n",
        "          f\"{r['diff']:>+8.4f} | {r['cohens_d']:>+7.3f} | {r['p_value']:>10.2e}{sig} | {hyp}\")\n",
        "\n",
        "print(\"-\" * 100)\n",
        "print(f\"\\nConfirmed: {sum(1 for r in stats_results if r['confirmed'])}/{len(stats_results)}\")\n",
        "print(f\"Significant: {sum(1 for r in stats_results if r['significant'])}/{len(stats_results)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oddXUPGQfdM5"
      },
      "outputs": [],
      "source": [
        "print(\"\\nMETRICS RANKED BY EFFECT SIZE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, r in enumerate(sorted(stats_results, key=lambda x: abs(x[\"cohens_d\"]), reverse=True), 1):\n",
        "    d = abs(r[\"cohens_d\"])\n",
        "    effect = \"LARGE\" if d >= 0.8 else \"MEDIUM\" if d >= 0.5 else \"SMALL\" if d >= 0.2 else \"negligible\"\n",
        "    sig = \"***\" if r[\"p_value\"] < 0.001 else \"**\" if r[\"p_value\"] < 0.01 else \"*\" if r[\"p_value\"] < 0.05 else \"\"\n",
        "    print(f\"{i:2}. {r['metric']:<35} | d = {r['cohens_d']:>+.3f} ({effect:<10}) | {sig}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVbd69rFfdM5"
      },
      "source": [
        "## 5. Classification Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2ZD0lJsfdM5"
      },
      "outputs": [],
      "source": [
        "def compute_risk_score(row):\n",
        "    launch_ctx_norm = np.clip((row[\"launch_from_context\"] - 0.3) / 0.7, 0, 1)\n",
        "    mean_ctx_norm = np.clip((row[\"mean_distance_from_context\"] - 0.3) / 0.7, 0, 1)\n",
        "    grounding_norm = np.clip(row[\"grounding_ratio\"] - 0.5, 0, 1)\n",
        "    return 0.4 * launch_ctx_norm + 0.3 * mean_ctx_norm + 0.3 * grounding_norm\n",
        "\n",
        "df[\"risk_score\"] = df.apply(compute_risk_score, axis=1)\n",
        "\n",
        "y_true = (~df[\"is_valid\"]).astype(int)\n",
        "y_scores = df[\"risk_score\"]\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "roc_auc = roc_auc_score(y_true, y_scores)\n",
        "\n",
        "j_scores = tpr - fpr\n",
        "optimal_idx = np.argmax(j_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "\n",
        "y_pred = (y_scores >= optimal_threshold).astype(int)\n",
        "accuracy = (y_pred == y_true).mean()\n",
        "\n",
        "tp = ((y_pred == 1) & (y_true == 1)).sum()\n",
        "fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
        "fn = ((y_pred == 0) & (y_true == 1)).sum()\n",
        "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "print(f\"Optimal threshold: {optimal_threshold:.4f}\")\n",
        "print(f\"\\nAccuracy: {accuracy:.1%}, Precision: {precision:.1%}, Recall: {recall:.1%}, F1: {f1:.1%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkFHXShefdM5"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "ax = axes[0]\n",
        "ax.plot(fpr, tpr, color=\"#3498db\", linewidth=2, label=f\"ROC (AUC = {roc_auc:.3f})\")\n",
        "ax.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
        "ax.scatter(fpr[optimal_idx], tpr[optimal_idx], color=\"red\", s=100, zorder=5)\n",
        "ax.set_xlabel(\"False Positive Rate\"); ax.set_ylabel(\"True Positive Rate\")\n",
        "ax.set_title(\"ROC Curve\"); ax.legend(); ax.grid(True, alpha=0.3)\n",
        "\n",
        "ax = axes[1]\n",
        "ax.hist(df[df[\"is_valid\"]][\"risk_score\"], bins=30, alpha=0.6, label=\"Valid\", color=\"#2ecc71\", density=True)\n",
        "ax.hist(df[~df[\"is_valid\"]][\"risk_score\"], bins=30, alpha=0.6, label=\"Hallucinated\", color=\"#e74c3c\", density=True)\n",
        "ax.axvline(optimal_threshold, color=\"black\", linestyle=\"--\", linewidth=2)\n",
        "ax.set_xlabel(\"Risk Score\"); ax.set_ylabel(\"Density\")\n",
        "ax.set_title(\"Score Distribution\"); ax.legend(); ax.grid(True, alpha=0.3)\n",
        "\n",
        "ax = axes[2]\n",
        "ctx_effects = [next(r[\"cohens_d\"] for r in stats_results if r[\"metric\"] == m)\n",
        "               for m in [\"launch_from_context\", \"mean_distance_from_context\"]]\n",
        "q_effects = [next(r[\"cohens_d\"] for r in stats_results if r[\"metric\"] == m)\n",
        "             for m in [\"launch_from_question\", \"mean_distance_from_question\"]]\n",
        "x = np.arange(2); width = 0.35\n",
        "ax.bar(x - width/2, ctx_effects, width, label=\"Context-anchored\", color=\"#3498db\")\n",
        "ax.bar(x + width/2, q_effects, width, label=\"Question-anchored\", color=\"#e67e22\")\n",
        "ax.set_xticks(x); ax.set_xticklabels([\"Launch\", \"Mean Distance\"])\n",
        "ax.set_ylabel(\"Cohen's d\"); ax.set_title(\"Effect Size Comparison\")\n",
        "ax.legend(); ax.axhline(0, color=\"black\", linewidth=0.5); ax.grid(True, alpha=0.3, axis=\"y\")\n",
        "\n",
        "plt.suptitle(f\"Classification (n={len(df)}, AUC={roc_auc:.3f})\", fontsize=14)\n",
        "plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5POdbvlffdM5"
      },
      "source": [
        "## 6. Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBT_-ytwfdM6"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"EXPERIMENT SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "launch_ctx = next(r for r in stats_results if r[\"metric\"] == \"launch_from_context\")\n",
        "\n",
        "print(f\"\"\"\n",
        "Dataset: HaluEval QA (with context/knowledge grounding)\n",
        "Samples: {len(df)} ({len(df[df['is_valid']])} valid, {len(df[~df['is_valid']])} hallucinated)\n",
        "\n",
        "PRIMARY METRIC - launch_from_context:\n",
        "  Valid: {launch_ctx['valid_mean']:.4f}\n",
        "  Halluc: {launch_ctx['halluc_mean']:.4f}\n",
        "  Cohen's d: {launch_ctx['cohens_d']:+.3f}\n",
        "  p-value: {launch_ctx['p_value']:.2e}\n",
        "  Hypothesis confirmed: {launch_ctx['confirmed']}\n",
        "\n",
        "Classification: ROC-AUC = {roc_auc:.4f}, F1 = {f1:.1%}\n",
        "\"\"\")\n",
        "\n",
        "if launch_ctx['confirmed'] and roc_auc > 0.6:\n",
        "    print(\"ASSESSMENT: HYPOTHESIS CONFIRMED - Context-anchored analysis detects fabrications.\")\n",
        "elif roc_auc > 0.55:\n",
        "    print(\"ASSESSMENT: WEAK SIGNAL - Some discrimination but small effect sizes.\")\n",
        "else:\n",
        "    print(\"ASSESSMENT: NO SIGNAL - Geometric trajectory approach not viable for hallucination detection.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}