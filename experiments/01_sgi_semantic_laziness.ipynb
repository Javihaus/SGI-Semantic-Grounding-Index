{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEL4cuhBSeRI"
      },
      "source": [
        "# SGI Semantic Laziness: Main Paper Experiments\n",
        "\n",
        "This notebook reproduces the main experimental results from:\n",
        "\n",
        "> Marín, J. (2026). \"Semantic Grounding Index: Geometric Bounds on Context Engagement in RAG Systems\" [arXiv:2512.13771](https://arxiv.org/abs/2512.13771)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xko3Rw-PSkRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLT4ZZ2xSeRK"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS-64m5qSeRK"
      },
      "outputs": [],
      "source": [
        "# Uncomment to install dependencies\n",
        "# !pip install -q datasets sentence-transformers numpy pandas matplotlib seaborn scipy scikit-learn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8fCTNKhSeRL"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy import stats\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "# Local imports\n",
        "from sgi import (\n",
        "    compute_sgi,\n",
        "    load_halueval_qa,\n",
        "    load_truthfulqa,\n",
        "    print_dataset_summary,\n",
        "    compute_effect_size,\n",
        "    compute_cohens_d,\n",
        "    compute_correlation_matrix,\n",
        "    compute_pairwise_correlations,\n",
        "    compute_calibration,\n",
        "    compute_stratified_analysis,\n",
        "    compute_subgroup_analysis,\n",
        "    set_publication_style,\n",
        "    plot_correlation_heatmap,\n",
        ")\n",
        "\n",
        "set_publication_style()\n",
        "print('Setup complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa44PZwiSeRL"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tO-AxTp5SeRL"
      },
      "outputs": [],
      "source": [
        "# Paper configuration: n=5,000 samples\n",
        "MAX_SAMPLES = 5000\n",
        "\n",
        "# Embedding models tested in the paper\n",
        "EMBEDDING_MODELS = {\n",
        "    'mpnet': 'all-mpnet-base-v2',       # 768d, Sentence-Transformers\n",
        "    'minilm': 'all-MiniLM-L6-v2',       # 384d, Sentence-Transformers (distilled)\n",
        "    'bge': 'BAAI/bge-base-en-v1.5',     # 768d, BAAI contrastive\n",
        "    'e5': 'intfloat/e5-base-v2',        # 768d, Microsoft instruction-tuned\n",
        "    'gte': 'thenlper/gte-base',         # 768d, Alibaba general text\n",
        "}\n",
        "\n",
        "print(f'Configuration:')\n",
        "print(f'  MAX_SAMPLES: {MAX_SAMPLES}')\n",
        "print(f'  Models: {list(EMBEDDING_MODELS.keys())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBaVB1VnSeRM"
      },
      "source": [
        "## Section 1: Data Loading\n",
        "\n",
        "Load HaluEval QA dataset with stratified sampling by hallucination label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0W0Tm83XSeRM"
      },
      "outputs": [],
      "source": [
        "# Load HaluEval QA dataset\n",
        "cases = load_halueval_qa(max_samples=MAX_SAMPLES)\n",
        "print_dataset_summary(cases, 'HaluEval QA')\n",
        "\n",
        "# Show example\n",
        "print('\\n=== EXAMPLE ===')\n",
        "ex = cases[0]\n",
        "print(f'Question: {ex.question[:100]}...')\n",
        "print(f'Context: {ex.context[:150]}...')\n",
        "print(f'Response: {ex.response[:150]}...')\n",
        "print(f'Is Grounded: {ex.is_grounded}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSQsL1XfSeRM"
      },
      "source": [
        "## Section 2: SGI Computation Across Models\n",
        "\n",
        "For each embedding model, compute:\n",
        "$$\\text{SGI} = \\frac{\\theta(r, q)}{\\theta(r, c)}$$\n",
        "\n",
        "where $\\theta$ is angular distance and $r, q, c$ are L2-normalized embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18aVm7_sSeRM"
      },
      "outputs": [],
      "source": [
        "def compute_sgi_for_model(cases, model_name: str, model_id: str) -> pd.DataFrame:\n",
        "    \"\"\"Compute SGI for all cases using specified embedding model.\"\"\"\n",
        "    print(f'\\n{\"=\"*60}')\n",
        "    print(f'Model: {model_name} ({model_id})')\n",
        "    print(f'{\"=\"*60}')\n",
        "\n",
        "    encoder = SentenceTransformer(model_id)\n",
        "    print(f'Embedding dimension: {encoder.get_sentence_embedding_dimension()}')\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for case in tqdm(cases, desc=f'Computing SGI ({model_name})'):\n",
        "        try:\n",
        "            q_emb = encoder.encode(case.question)\n",
        "            c_emb = encoder.encode(case.context)\n",
        "            r_emb = encoder.encode(case.response)\n",
        "\n",
        "            sgi_result = compute_sgi(q_emb, c_emb, r_emb)\n",
        "\n",
        "            results.append({\n",
        "                'id': case.id,\n",
        "                'is_grounded': case.is_grounded,\n",
        "                'question_length': len(case.question),\n",
        "                'context_length': len(case.context),\n",
        "                'response_length': len(case.response),\n",
        "                f'sgi_{model_name}': sgi_result.sgi,\n",
        "                f'theta_rq_{model_name}': sgi_result.theta_rq,\n",
        "                f'theta_rc_{model_name}': sgi_result.theta_rc,\n",
        "                f'theta_qc_{model_name}': sgi_result.theta_qc,\n",
        "            })\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    # Clean up\n",
        "    del encoder\n",
        "    gc.collect()\n",
        "\n",
        "    print(f'Processed: {len(results)} samples')\n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mu1GOPBKSeRM"
      },
      "outputs": [],
      "source": [
        "# Run analysis for each model\n",
        "model_dfs = {}\n",
        "\n",
        "for model_name, model_id in EMBEDDING_MODELS.items():\n",
        "    model_dfs[model_name] = compute_sgi_for_model(cases, model_name, model_id)\n",
        "\n",
        "print(f'\\n\\nCompleted analysis for {len(model_dfs)} models.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5BjbDgCSeRM"
      },
      "outputs": [],
      "source": [
        "# Merge results into single dataframe\n",
        "model_names = list(model_dfs.keys())\n",
        "merged_df = model_dfs[model_names[0]][['id', 'is_grounded', 'question_length', 'context_length', 'response_length']].copy()\n",
        "\n",
        "for model_name, df in model_dfs.items():\n",
        "    cols_to_merge = ['id'] + [c for c in df.columns if model_name in c]\n",
        "    merged_df = merged_df.merge(df[cols_to_merge], on='id', how='inner')\n",
        "\n",
        "print(f'Merged: {len(merged_df)} samples with SGI from {len(model_names)} models')\n",
        "print(f'  Grounded: {merged_df[\"is_grounded\"].sum()}')\n",
        "print(f'  Hallucinated: {(~merged_df[\"is_grounded\"]).sum()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SEQRRl7SeRN"
      },
      "source": [
        "## Section 3: Table 1 - Cross-Model Effect Sizes\n",
        "\n",
        "Reproducing Table 1 from the paper:\n",
        "\n",
        "| Model | SGI (Valid) | SGI (Halluc) | Cohen's d | AUC | p-value |\n",
        "|-------|-------------|--------------|-----------|-----|---------|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oaYgMwFSeRN"
      },
      "outputs": [],
      "source": [
        "print('='*80)\n",
        "print('TABLE 1: CROSS-MODEL EFFECT SIZES (n={})'.format(len(merged_df)))\n",
        "print('='*80)\n",
        "print(f'\\n{\"Model\":<10} | {\"SGI (Valid)\":>12} | {\"SGI (Halluc)\":>12} | {\"Cohen\\'s d\":>10} | {\"AUROC\":>8} | {\"p-value\":>12}')\n",
        "print('-'*80)\n",
        "\n",
        "effect_sizes = {}\n",
        "table1_results = []\n",
        "\n",
        "for model_name in model_names:\n",
        "    sgi_col = f'sgi_{model_name}'\n",
        "    values = merged_df[sgi_col].values\n",
        "    labels = merged_df['is_grounded'].values\n",
        "\n",
        "    result = compute_effect_size(values, labels, model_name)\n",
        "    effect_sizes[model_name] = result.to_dict()\n",
        "\n",
        "    sig = '***' if result.p_value < 0.001 else '**' if result.p_value < 0.01 else '*' if result.p_value < 0.05 else ''\n",
        "    print(f'{model_name:<10} | {result.grounded_mean:>12.3f} | {result.hallucinated_mean:>12.3f} | '\n",
        "          f'{result.cohens_d:>+10.2f} | {result.auroc:>8.3f} | {result.p_value:>10.2e} {sig}')\n",
        "\n",
        "    table1_results.append({\n",
        "        'Model': model_name,\n",
        "        'SGI (Valid)': result.grounded_mean,\n",
        "        'SGI (Halluc)': result.hallucinated_mean,\n",
        "        \"Cohen's d\": result.cohens_d,\n",
        "        'AUROC': result.auroc,\n",
        "        'p-value': result.p_value,\n",
        "    })\n",
        "\n",
        "print('-'*80)\n",
        "\n",
        "# Summary statistics\n",
        "d_values = [r['cohens_d'] for r in effect_sizes.values()]\n",
        "auc_values = [r['auroc'] for r in effect_sizes.values()]\n",
        "valid_means = [r['grounded_mean'] for r in effect_sizes.values()]\n",
        "halluc_means = [r['hallucinated_mean'] for r in effect_sizes.values()]\n",
        "\n",
        "print(f'{\"Mean\":<10} | {np.mean(valid_means):>12.3f} | {np.mean(halluc_means):>12.3f} | '\n",
        "      f'{np.mean(d_values):>+10.2f} | {np.mean(auc_values):>8.3f} | {\"—\":>12}')\n",
        "print('='*80)\n",
        "\n",
        "print(f'\\nSummary:')\n",
        "print(f\"  Cohen's d: mean={np.mean(d_values):.2f}, range=[{min(d_values):.2f}, {max(d_values):.2f}]\")\n",
        "print(f'  AUROC: mean={np.mean(auc_values):.3f}, range=[{min(auc_values):.3f}, {max(auc_values):.3f}]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68XHT0t9SeRN"
      },
      "source": [
        "## Section 4: Figure 2 - Cross-Model Correlation Matrix\n",
        "\n",
        "Compute Pearson correlation between SGI scores from different models.\n",
        "Target: mean off-diagonal r ≈ 0.85"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WvWPAu-SeRN"
      },
      "outputs": [],
      "source": [
        "print('='*70)\n",
        "print('FIGURE 2: CROSS-MODEL CORRELATION MATRIX')\n",
        "print('='*70)\n",
        "\n",
        "sgi_cols = [f'sgi_{m}' for m in model_names]\n",
        "sgi_df = merged_df[sgi_cols].copy()\n",
        "sgi_df.columns = model_names\n",
        "\n",
        "# Pearson correlation\n",
        "pearson_matrix = compute_correlation_matrix(sgi_df, model_names, method='pearson')\n",
        "pearson_stats = compute_pairwise_correlations(pearson_matrix)\n",
        "\n",
        "# Spearman correlation\n",
        "spearman_matrix = compute_correlation_matrix(sgi_df, model_names, method='spearman')\n",
        "spearman_stats = compute_pairwise_correlations(spearman_matrix)\n",
        "\n",
        "print('\\nPearson Correlation Matrix:')\n",
        "print(pearson_matrix.round(3).to_string())\n",
        "print(f'\\nOff-diagonal Pearson r: mean={pearson_stats[\"mean\"]:.3f}, range=[{pearson_stats[\"min\"]:.3f}, {pearson_stats[\"max\"]:.3f}]')\n",
        "\n",
        "print('\\n\\nSpearman Correlation Matrix:')\n",
        "print(spearman_matrix.round(3).to_string())\n",
        "print(f'\\nOff-diagonal Spearman ρ: mean={spearman_stats[\"mean\"]:.3f}, range=[{spearman_stats[\"min\"]:.3f}, {spearman_stats[\"max\"]:.3f}]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtChPDvrSeRN"
      },
      "outputs": [],
      "source": [
        "# Plot correlation heatmap\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "plot_correlation_heatmap(pearson_matrix, axes[0], title='Pearson Correlation (Linear Agreement)')\n",
        "plot_correlation_heatmap(spearman_matrix, axes[1], title='Spearman Correlation (Ranking Agreement)')\n",
        "\n",
        "fig.suptitle(f'Figure 2: Cross-Model SGI Correlation (n={len(merged_df)})', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('figure2_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHYCGkHCSeRN"
      },
      "source": [
        "## Section 5: Table 2 - Stratified Analysis by θ(q,c)\n",
        "\n",
        "Effect size should increase monotonically with question-context separation, confirming the triangle inequality prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SguGwPVySeRN"
      },
      "outputs": [],
      "source": [
        "print('='*90)\n",
        "print('TABLE 2: STRATIFIED ANALYSIS BY θ(q,c) TERCILE')\n",
        "print('='*90)\n",
        "\n",
        "# Use primary model (mpnet) for stratified analysis\n",
        "primary_model = 'mpnet'\n",
        "analysis_df = merged_df[['is_grounded', f'sgi_{primary_model}', f'theta_qc_{primary_model}']].copy()\n",
        "analysis_df.columns = ['is_grounded', 'sgi', 'theta_qc']\n",
        "\n",
        "stratified_results = compute_stratified_analysis(\n",
        "    df=analysis_df,\n",
        "    stratify_col='theta_qc',\n",
        "    sgi_col='sgi',\n",
        "    label_col='is_grounded',\n",
        "    n_bins=3\n",
        ")\n",
        "\n",
        "print(f'\\n{\"θ(q,c) Tercile\":<15} | {\"n\":>6} | {\"θ(q,c) Range\":>18} | {\"SGI (Valid)\":>12} | {\"SGI (Halluc)\":>12} | {\"Cohen\\'s d\":>10} | {\"AUROC\":>8}')\n",
        "print('-'*95)\n",
        "\n",
        "for _, row in stratified_results.iterrows():\n",
        "    range_str = f'[{row[\"range_min\"]:.2f}, {row[\"range_max\"]:.2f}]'\n",
        "    print(f'{row[\"stratum\"]:<15} | {row[\"n\"]:>6} | {range_str:>18} | {row[\"sgi_grounded\"]:>12.2f} | '\n",
        "          f'{row[\"sgi_hallucinated\"]:>12.2f} | {row[\"cohens_d\"]:>+10.2f} | {row[\"auroc\"]:>8.3f}')\n",
        "\n",
        "print('-'*95)\n",
        "print('\\nKey insight: Effect size INCREASES monotonically with θ(q,c), confirming triangle inequality prediction.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzPd0SgjSeRO"
      },
      "source": [
        "## Section 6: Table 3 - Subgroup Analysis\n",
        "\n",
        "Analyze effect sizes by question length, context length, and response length terciles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MqKsJipSeRO"
      },
      "outputs": [],
      "source": [
        "print('='*90)\n",
        "print('TABLE 3: SUBGROUP ANALYSIS BY TEXT LENGTH')\n",
        "print('='*90)\n",
        "\n",
        "# Prepare analysis dataframe\n",
        "subgroup_df = merged_df[['is_grounded', f'sgi_{primary_model}',\n",
        "                         'question_length', 'context_length', 'response_length']].copy()\n",
        "subgroup_df.columns = ['is_grounded', 'sgi', 'question_length', 'context_length', 'response_length']\n",
        "\n",
        "all_subgroup_results = []\n",
        "\n",
        "for feature in ['question_length', 'context_length', 'response_length']:\n",
        "    feature_results = compute_subgroup_analysis(\n",
        "        df=subgroup_df,\n",
        "        feature_col=feature,\n",
        "        sgi_col='sgi',\n",
        "        label_col='is_grounded',\n",
        "        n_bins=3\n",
        "    )\n",
        "    all_subgroup_results.append(feature_results)\n",
        "\n",
        "combined_subgroup = pd.concat(all_subgroup_results, ignore_index=True)\n",
        "\n",
        "print(f'\\n{\"Feature\":<20} | {\"Subgroup\":<8} | {\"n\":>6} | {\"Cohen\\'s d\":>10} | {\"AUROC\":>8}')\n",
        "print('-'*65)\n",
        "\n",
        "for _, row in combined_subgroup.iterrows():\n",
        "    print(f'{row[\"feature\"]:<20} | {row[\"subgroup\"]:<8} | {row[\"n\"]:>6} | {row[\"cohens_d\"]:>+10.2f} | {row[\"auroc\"]:>8.3f}')\n",
        "\n",
        "print('-'*65)\n",
        "print('\\nKey findings:')\n",
        "print('  - Response length: d increases from Short to Long (stronger signal for longer responses)')\n",
        "print('  - Question length: d decreases from Short to Long')\n",
        "print('  - Context length: d relatively stable')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3VjnIw7SeRO"
      },
      "source": [
        "## Section 7: Calibration Analysis (Figure 5)\n",
        "\n",
        "Compute Expected Calibration Error (ECE) and plot reliability diagram.\n",
        "Target: ECE ≈ 0.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8_Z9PtkSeRO"
      },
      "outputs": [],
      "source": [
        "print('='*70)\n",
        "print('FIGURE 5: CALIBRATION ANALYSIS')\n",
        "print('='*70)\n",
        "\n",
        "# Compute calibration for primary model\n",
        "sgi_scores = merged_df[f'sgi_{primary_model}'].values\n",
        "labels = merged_df['is_grounded'].values\n",
        "\n",
        "ece, bin_accuracies, bin_confidences, bin_counts = compute_calibration(\n",
        "    scores=sgi_scores,\n",
        "    labels=labels,\n",
        "    n_bins=10\n",
        ")\n",
        "\n",
        "print(f'\\nExpected Calibration Error (ECE): {ece:.3f}')\n",
        "print(f'\\nBin-wise analysis:')\n",
        "print(f'{\"Bin\":>4} | {\"Count\":>8} | {\"Confidence\":>12} | {\"Accuracy\":>10} | {\"Gap\":>8}')\n",
        "print('-'*55)\n",
        "\n",
        "for i, (conf, acc, count) in enumerate(zip(bin_confidences, bin_accuracies, bin_counts)):\n",
        "    gap = abs(conf - acc)\n",
        "    print(f'{i+1:>4} | {count:>8} | {conf:>12.3f} | {acc:>10.3f} | {gap:>8.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifzTAC2PSeRO"
      },
      "outputs": [],
      "source": [
        "# Plot calibration figures\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Reliability diagram\n",
        "ax = axes[0]\n",
        "ax.bar(np.arange(len(bin_confidences)), bin_accuracies, width=0.8, alpha=0.7, label='Actual', color='#3498db')\n",
        "ax.plot(np.arange(len(bin_confidences)), bin_confidences, 'ro-', label='Predicted', markersize=8)\n",
        "ax.plot([-0.5, len(bin_confidences)-0.5], [0, 1], 'k--', alpha=0.3, label='Perfect calibration')\n",
        "ax.set_xlabel('Bin')\n",
        "ax.set_ylabel('Hallucination Rate')\n",
        "ax.set_title(f'Reliability Diagram (ECE = {ece:.3f})')\n",
        "ax.legend()\n",
        "ax.set_xlim(-0.5, len(bin_confidences)-0.5)\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "# Hallucination rate by SGI decile\n",
        "ax = axes[1]\n",
        "deciles = pd.qcut(merged_df[f'sgi_{primary_model}'], 10, labels=False, duplicates='drop')\n",
        "halluc_rates = []\n",
        "for d in range(10):\n",
        "    mask = deciles == d\n",
        "    if mask.sum() > 0:\n",
        "        rate = (~merged_df.loc[mask, 'is_grounded']).mean()\n",
        "        halluc_rates.append(rate)\n",
        "    else:\n",
        "        halluc_rates.append(0)\n",
        "\n",
        "ax.bar(range(len(halluc_rates)), halluc_rates, color='#e74c3c', alpha=0.7)\n",
        "ax.set_xlabel('SGI Decile (1=lowest, 10=highest)')\n",
        "ax.set_ylabel('Hallucination Rate')\n",
        "ax.set_title('Hallucination Rate by SGI Decile')\n",
        "ax.set_xticks(range(len(halluc_rates)))\n",
        "ax.set_xticklabels([str(i+1) for i in range(len(halluc_rates))])\n",
        "\n",
        "fig.suptitle('Figure 5: Calibration Analysis', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('figure5_calibration.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRP1HYRdSeRO"
      },
      "source": [
        "## Section 8: TruthfulQA Negative Result (Table 4)\n",
        "\n",
        "SGI should NOT work on TruthfulQA because angular geometry measures topical engagement, not factual accuracy.\n",
        "Expected: AUC ≈ 0.478 (below chance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19B09mg6SeRO"
      },
      "outputs": [],
      "source": [
        "print('='*70)\n",
        "print('TABLE 4: TRUTHFULQA NEGATIVE RESULT')\n",
        "print('='*70)\n",
        "\n",
        "# Load TruthfulQA\n",
        "truthfulqa_cases = load_truthfulqa(max_samples=2000)\n",
        "print_dataset_summary(truthfulqa_cases, 'TruthfulQA')\n",
        "\n",
        "# Compute SGI for TruthfulQA using primary model\n",
        "encoder = SentenceTransformer(EMBEDDING_MODELS[primary_model])\n",
        "\n",
        "truthfulqa_results = []\n",
        "for case in tqdm(truthfulqa_cases, desc='Computing SGI for TruthfulQA'):\n",
        "    try:\n",
        "        q_emb = encoder.encode(case.question)\n",
        "        c_emb = encoder.encode(case.context)\n",
        "        r_emb = encoder.encode(case.response)\n",
        "\n",
        "        sgi_result = compute_sgi(q_emb, c_emb, r_emb)\n",
        "\n",
        "        truthfulqa_results.append({\n",
        "            'id': case.id,\n",
        "            'is_grounded': case.is_grounded,\n",
        "            'sgi': sgi_result.sgi,\n",
        "        })\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "del encoder\n",
        "gc.collect()\n",
        "\n",
        "truthfulqa_df = pd.DataFrame(truthfulqa_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csjdgoFMSeRO"
      },
      "outputs": [],
      "source": [
        "# Compute effect size for TruthfulQA\n",
        "tqa_values = truthfulqa_df['sgi'].values\n",
        "tqa_labels = truthfulqa_df['is_grounded'].values\n",
        "\n",
        "tqa_result = compute_effect_size(tqa_values, tqa_labels, 'TruthfulQA')\n",
        "\n",
        "print(f'\\nTruthfulQA Results (n={len(truthfulqa_df)}):')\n",
        "print(f'  SGI (Truthful): {tqa_result.grounded_mean:.3f}')\n",
        "print(f'  SGI (False): {tqa_result.hallucinated_mean:.3f}')\n",
        "print(f\"  Cohen's d: {tqa_result.cohens_d:+.3f}\")\n",
        "print(f'  AUROC: {tqa_result.auroc:.3f}')\n",
        "print(f'  p-value: {tqa_result.p_value:.3f}')\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "if tqa_result.auroc < 0.55:\n",
        "    print('EXPECTED RESULT: SGI fails on TruthfulQA (AUC near/below 0.5)')\n",
        "    print('This confirms that angular geometry measures TOPICAL ENGAGEMENT,')\n",
        "    print('not FACTUAL ACCURACY. Misconceptions are semantically close to questions.')\n",
        "else:\n",
        "    print('UNEXPECTED: SGI shows some signal on TruthfulQA.')\n",
        "print('='*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7wLmjTaSeRO"
      },
      "source": [
        "## Section 9: Signal Decomposition (Table 5)\n",
        "\n",
        "Analyze which component drives the SGI signal: θ(r,q) or θ(r,c)?\n",
        "\n",
        "Key insight: Semantic laziness is driven by hallucinations being CLOSER to questions, not farther from contexts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMgzFn4CSeRP"
      },
      "outputs": [],
      "source": [
        "print('='*70)\n",
        "print('TABLE 5: SIGNAL DECOMPOSITION')\n",
        "print('='*70)\n",
        "print(f'\\n{\"Model\":<10} | {\"d(θr,q)\":>10} | {\"d(θr,c)\":>10} | Primary Driver')\n",
        "print('-'*55)\n",
        "\n",
        "decomposition_results = []\n",
        "\n",
        "for model_name in model_names:\n",
        "    theta_rq_col = f'theta_rq_{model_name}'\n",
        "    theta_rc_col = f'theta_rc_{model_name}'\n",
        "    labels = merged_df['is_grounded'].values\n",
        "\n",
        "    # Note: For theta_rq and theta_rc, lower values for hallucinations means they're closer\n",
        "    d_rq = compute_effect_size(merged_df[theta_rq_col].values, labels, 'θ_rq',\n",
        "                               positive_class_is_hallucinated=False)\n",
        "    d_rc = compute_effect_size(merged_df[theta_rc_col].values, labels, 'θ_rc',\n",
        "                               positive_class_is_hallucinated=False)\n",
        "\n",
        "    driver = 'θ(r,q)' if abs(d_rq.cohens_d) > abs(d_rc.cohens_d) else 'θ(r,c)'\n",
        "    print(f'{model_name:<10} | {d_rq.cohens_d:>+10.2f} | {d_rc.cohens_d:>+10.2f} | {driver}')\n",
        "\n",
        "    decomposition_results.append({\n",
        "        'Model': model_name,\n",
        "        'd(θr,q)': d_rq.cohens_d,\n",
        "        'd(θr,c)': d_rc.cohens_d,\n",
        "        'Primary Driver': driver\n",
        "    })\n",
        "\n",
        "print('-'*55)\n",
        "print('\\nInterpretation:')\n",
        "print('  • Negative d(θ_rq): Hallucinations are CLOSER to question')\n",
        "print('  • Positive d(θ_rc): Hallucinations are FARTHER from context')\n",
        "print('  • Both effects together = \"semantic laziness\" signature')\n",
        "print('  • θ(r,q) is the PRIMARY driver across all models')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}