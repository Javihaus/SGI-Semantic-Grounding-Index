{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# TruthfulQA: Establishing SGI's Limitations\n",
    "\n",
    "This notebook demonstrates a critical **NEGATIVE result**: SGI cannot discriminate factual accuracy, only context engagement.\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Understanding what a metric **cannot** detect is just as important as understanding what it can detect. This notebook establishes clear boundaries on what geometric methods can and cannot accomplish.\n",
    "\n",
    "## The Hypothesis\n",
    "\n",
    "TruthfulQA tests *misconceptions* — plausible-sounding false beliefs that are commonly held.\n",
    "\n",
    "**Example:**\n",
    "- Q: \"What causes the seasons?\"\n",
    "- Truthful: \"Axial tilt causes seasons\"\n",
    "- False: \"Distance from the sun causes seasons\"\n",
    "\n",
    "Both answers are **topically identical** — they discuss seasons, Earth, and causation. Angular distance cannot distinguish them.\n",
    "\n",
    "**SGI measures:** \"Did the response engage with provided context?\"\n",
    "\n",
    "**SGI does NOT measure:** \"Is this factually correct?\"\n",
    "\n",
    "## Expected Result\n",
    "\n",
    "- **AUC ≈ 0.478** (below chance)\n",
    "- **Cohen's d ≈ -0.14** (negligible effect)\n",
    "\n",
    "---\n",
    "\n",
    "**Reference:** Marín (2024) \"Semantic Grounding Index\" [arXiv:2512.13771](https://arxiv.org/abs/2512.13771) Section 5.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install dependencies\n",
    "# !pip install -q datasets sentence-transformers numpy pandas matplotlib seaborn scipy scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy import stats\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "from sgi import (\n",
    "    compute_sgi,\n",
    "    load_truthfulqa,\n",
    "    print_dataset_summary,\n",
    "    compute_effect_size,\n",
    "    compute_cohens_d,\n",
    "    set_publication_style,\n",
    ")\n",
    "\n",
    "set_publication_style()\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = 'all-mpnet-base-v2'\n",
    "MAX_SAMPLES = 2000\n",
    "\n",
    "print(f'Configuration:')\n",
    "print(f'  Model: {MODEL_NAME}')\n",
    "print(f'  MAX_SAMPLES: {MAX_SAMPLES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Load TruthfulQA Dataset\n",
    "\n",
    "TruthfulQA contains questions designed to elicit false beliefs. Unlike HaluEval, there is **no external context** — the \"context\" is essentially the same as the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TruthfulQA\n",
    "cases = load_truthfulqa(max_samples=MAX_SAMPLES)\n",
    "print_dataset_summary(cases, 'TruthfulQA')\n",
    "\n",
    "# Show examples\n",
    "print('\\n' + '='*70)\n",
    "print('EXAMPLE CASES')\n",
    "print('='*70)\n",
    "\n",
    "truthful_ex = next(c for c in cases if c.is_grounded)\n",
    "false_ex = next(c for c in cases if not c.is_grounded)\n",
    "\n",
    "print('\\nTRUTHFUL EXAMPLE:')\n",
    "print(f'  Question: {truthful_ex.question[:100]}...')\n",
    "print(f'  Response: {truthful_ex.response[:150]}...')\n",
    "\n",
    "print('\\nFALSE EXAMPLE:')\n",
    "print(f'  Question: {false_ex.question[:100]}...')\n",
    "print(f'  Response: {false_ex.response[:150]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Compute SGI for TruthfulQA\n",
    "\n",
    "We compute SGI using the standard formula:\n",
    "\n",
    "$$\\text{SGI} = \\frac{\\theta(r, q)}{\\theta(r, c)}$$\n",
    "\n",
    "**Key insight:** In TruthfulQA, `context ≈ question`, so the geometric relationship is fundamentally different from RAG scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize encoder\n",
    "encoder = SentenceTransformer(MODEL_NAME)\n",
    "print(f'Model: {MODEL_NAME}')\n",
    "print(f'Embedding dimension: {encoder.get_sentence_embedding_dimension()}')\n",
    "\n",
    "# Compute SGI\n",
    "results = []\n",
    "\n",
    "for case in tqdm(cases, desc='Computing SGI for TruthfulQA'):\n",
    "    try:\n",
    "        q_emb = encoder.encode(case.question)\n",
    "        c_emb = encoder.encode(case.context)  # Note: context ≈ question in TruthfulQA\n",
    "        r_emb = encoder.encode(case.response)\n",
    "        \n",
    "        sgi_result = compute_sgi(q_emb, c_emb, r_emb)\n",
    "        \n",
    "        results.append({\n",
    "            'id': case.id,\n",
    "            'is_grounded': case.is_grounded,\n",
    "            'sgi': sgi_result.sgi,\n",
    "            'theta_rq': sgi_result.theta_rq,\n",
    "            'theta_rc': sgi_result.theta_rc,\n",
    "            'theta_qc': sgi_result.theta_qc,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "del encoder\n",
    "gc.collect()\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(f'\\nProcessed: {len(df)} samples')\n",
    "print(f'  Truthful: {df[\"is_grounded\"].sum()}')\n",
    "print(f'  False: {(~df[\"is_grounded\"]).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Table 4: TruthfulQA Results\n",
    "\n",
    "Expected: AUC ≈ 0.478 (below chance), confirming SGI cannot discriminate factual accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('TABLE 4: TRUTHFULQA NEGATIVE RESULT')\n",
    "print('='*70)\n",
    "\n",
    "# Separate groups\n",
    "truthful = df[df['is_grounded']]\n",
    "false = df[~df['is_grounded']]\n",
    "\n",
    "# Basic statistics\n",
    "print(f'\\n{\"Metric\":<20} | {\"Truthful\":>12} | {\"False\":>12} | {\"Difference\":>12}')\n",
    "print('-'*65)\n",
    "\n",
    "for metric in ['sgi', 'theta_rq', 'theta_rc', 'theta_qc']:\n",
    "    t_mean = truthful[metric].mean()\n",
    "    f_mean = false[metric].mean()\n",
    "    diff = f_mean - t_mean\n",
    "    print(f'{metric:<20} | {t_mean:>12.3f} | {f_mean:>12.3f} | {diff:>+12.3f}')\n",
    "\n",
    "print('-'*65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect size analysis\n",
    "print('\\n' + '='*70)\n",
    "print('EFFECT SIZE ANALYSIS')\n",
    "print('='*70)\n",
    "\n",
    "# Compute effect size for SGI\n",
    "sgi_result = compute_effect_size(\n",
    "    df['sgi'].values,\n",
    "    df['is_grounded'].values,\n",
    "    'SGI'\n",
    ")\n",
    "\n",
    "# Also compute for theta_rq (the key component)\n",
    "theta_rq_result = compute_effect_size(\n",
    "    df['theta_rq'].values,\n",
    "    df['is_grounded'].values,\n",
    "    'theta_rq'\n",
    ")\n",
    "\n",
    "print(f'\\nSGI Analysis:')\n",
    "print(f'  SGI (Truthful): {sgi_result.grounded_mean:.3f}')\n",
    "print(f'  SGI (False): {sgi_result.hallucinated_mean:.3f}')\n",
    "print(f\"  Cohen's d: {sgi_result.cohens_d:+.3f}\")\n",
    "print(f'  AUROC: {sgi_result.auroc:.3f}')\n",
    "print(f'  p-value: {sgi_result.p_value:.3f}')\n",
    "\n",
    "print(f'\\nθ(r,q) Analysis:')\n",
    "print(f'  θ(r,q) (Truthful): {theta_rq_result.grounded_mean:.3f}')\n",
    "print(f'  θ(r,q) (False): {theta_rq_result.hallucinated_mean:.3f}')\n",
    "print(f\"  Cohen's d: {theta_rq_result.cohens_d:+.3f}\")\n",
    "print(f'  AUROC: {theta_rq_result.auroc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper comparison\n",
    "print('\\n' + '-'*70)\n",
    "print('COMPARISON WITH PAPER EXPECTED VALUES')\n",
    "print('-'*70)\n",
    "\n",
    "print(f'''\\n{\"Metric\":<25} | {\"Obtained\":>12} | {\"Paper Expected\":>15} | {\"Match?\":>8}''')\n",
    "print('-'*70)\n",
    "\n",
    "# Check if results are in expected range\n",
    "auc_match = 'YES' if 0.42 < sgi_result.auroc < 0.55 else 'NO'\n",
    "d_match = 'YES' if -0.3 < sgi_result.cohens_d < 0.1 else 'NO'\n",
    "\n",
    "print(f'{\"AUROC\":<25} | {sgi_result.auroc:>12.3f} | {\"~0.478\":>15} | {auc_match:>8}')\n",
    "print(f\"{\"Cohen's d\":<25} | {sgi_result.cohens_d:>+12.3f} | {\"~-0.14\":>15} | {d_match:>8}\")\n",
    "print('-'*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Visualization: Why SGI Fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# SGI distributions\n",
    "ax = axes[0]\n",
    "ax.hist(truthful['sgi'], bins=30, alpha=0.6, label='Truthful', color='#2ecc71', density=True)\n",
    "ax.hist(false['sgi'], bins=30, alpha=0.6, label='False', color='#e74c3c', density=True)\n",
    "ax.axvline(truthful['sgi'].mean(), color='#27ae60', linestyle='--', linewidth=2)\n",
    "ax.axvline(false['sgi'].mean(), color='#c0392b', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel('SGI')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title(f\"SGI Distribution (d={sgi_result.cohens_d:.2f})\")\n",
    "ax.legend()\n",
    "\n",
    "# theta_rq distributions\n",
    "ax = axes[1]\n",
    "ax.hist(truthful['theta_rq'], bins=30, alpha=0.6, label='Truthful', color='#2ecc71', density=True)\n",
    "ax.hist(false['theta_rq'], bins=30, alpha=0.6, label='False', color='#e74c3c', density=True)\n",
    "ax.set_xlabel('θ(r,q)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title(f\"θ(r,q) Distribution\")\n",
    "ax.legend()\n",
    "\n",
    "# ROC curve\n",
    "ax = axes[2]\n",
    "y_true = (~df['is_grounded']).astype(int).values\n",
    "fpr, tpr, _ = roc_curve(y_true, df['sgi'].values)\n",
    "auroc = roc_auc_score(y_true, df['sgi'].values)\n",
    "\n",
    "ax.plot(fpr, tpr, color='#3498db', linewidth=2, label=f'SGI (AUC = {auroc:.3f})')\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random (AUC = 0.5)')\n",
    "ax.fill_between(fpr, tpr, alpha=0.2, color='#3498db')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve (Below Chance!)')\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "fig.suptitle('TruthfulQA: SGI Cannot Discriminate Factual Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('04_truthfulqa_negative_result.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Why Does SGI Fail on TruthfulQA?\n",
    "\n",
    "### The Geometric Explanation\n",
    "\n",
    "SGI measures the ratio of angular distances:\n",
    "\n",
    "$$\\text{SGI} = \\frac{\\theta(r, q)}{\\theta(r, c)}$$\n",
    "\n",
    "This captures **topical engagement**: whether the response moves toward the context or stays near the question.\n",
    "\n",
    "### TruthfulQA's Structure\n",
    "\n",
    "In TruthfulQA:\n",
    "1. **No external context exists** — the \"context\" is essentially the question itself\n",
    "2. **Truthful and false answers are topically identical** — both discuss the same subject matter\n",
    "3. **The difference is semantic, not topical** — accuracy vs. misconception\n",
    "\n",
    "### What SGI Can and Cannot Detect\n",
    "\n",
    "| Scenario | Can SGI Detect? | Why? |\n",
    "|----------|-----------------|------|\n",
    "| RAG hallucination (topic drift) | YES | Response geometrically distant from context |\n",
    "| Factual misconception | NO | Response topically aligned, just semantically wrong |\n",
    "| Context disengagement | YES | Clear geometric signature |\n",
    "| Subtle factual errors | NO | No geometric signature |\n",
    "\n",
    "### Implications\n",
    "\n",
    "This negative result is **valuable** because it:\n",
    "1. Defines the operational boundaries of geometric methods\n",
    "2. Prevents misuse in factuality detection\n",
    "3. Motivates complementary approaches (e.g., knowledge retrieval, fact verification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis: theta_qc distribution (should be very small in TruthfulQA)\n",
    "print('='*70)\n",
    "print('WHY SGI FAILS: THE GEOMETRY OF TRUTHFULQA')\n",
    "print('='*70)\n",
    "\n",
    "print(f'\\nθ(q,c) Statistics (Question-Context Separation):')\n",
    "print(f'  Mean: {df[\"theta_qc\"].mean():.4f}')\n",
    "print(f'  Std: {df[\"theta_qc\"].std():.4f}')\n",
    "print(f'  Min: {df[\"theta_qc\"].min():.4f}')\n",
    "print(f'  Max: {df[\"theta_qc\"].max():.4f}')\n",
    "\n",
    "print(f'''\\nInterpretation:\n",
    "  In TruthfulQA, θ(q,c) is very small because context ≈ question.\n",
    "  This collapses the geometric structure that SGI relies on.\n",
    "  \n",
    "  In HaluEval (where SGI works):\n",
    "    - Question and context are semantically distinct\n",
    "    - θ(q,c) provides \"room\" for responses to move toward context\n",
    "    - Hallucinations fail to make this geometric journey\n",
    "  \n",
    "  In TruthfulQA:\n",
    "    - Question and context are nearly identical\n",
    "    - No geometric \"space\" exists for SGI to measure engagement\n",
    "    - Both truthful and false answers are geometrically similar\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('TRUTHFULQA NEGATIVE RESULT - SUMMARY')\n",
    "print('='*70)\n",
    "\n",
    "print(f'''\n",
    "Dataset: TruthfulQA (n={len(df)})\n",
    "Model: {MODEL_NAME}\n",
    "\n",
    "RESULTS:\n",
    "  AUROC: {sgi_result.auroc:.3f} (expected ~0.478, below chance)\n",
    "  Cohen\\'s d: {sgi_result.cohens_d:+.3f} (expected ~-0.14, negligible)\n",
    "  p-value: {sgi_result.p_value:.3f}\n",
    "\n",
    "CONCLUSION:\n",
    "  SGI CANNOT discriminate factual accuracy.\n",
    "  \n",
    "  This is NOT a failure — it defines the operational boundary:\n",
    "  \n",
    "  SGI DETECTS:\n",
    "    - Context disengagement in RAG systems\n",
    "    - Topic drift / semantic laziness\n",
    "    - Hallucinations that ignore source material\n",
    "  \n",
    "  SGI DOES NOT DETECT:\n",
    "    - Factual errors within the correct topic\n",
    "    - Misconceptions expressed topically correctly\n",
    "    - Subtle semantic inaccuracies\n",
    "\n",
    "  For factual accuracy, use complementary methods:\n",
    "    - Knowledge retrieval systems\n",
    "    - Fact verification models\n",
    "    - Entailment classifiers\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "df.to_csv('04_truthfulqa_results.csv', index=False)\n",
    "\n",
    "summary = {\n",
    "    'n_samples': len(df),\n",
    "    'n_truthful': int(df['is_grounded'].sum()),\n",
    "    'n_false': int((~df['is_grounded']).sum()),\n",
    "    'auroc': float(sgi_result.auroc),\n",
    "    'cohens_d': float(sgi_result.cohens_d),\n",
    "    'p_value': float(sgi_result.p_value),\n",
    "    'sgi_truthful_mean': float(truthful['sgi'].mean()),\n",
    "    'sgi_false_mean': float(false['sgi'].mean()),\n",
    "    'theta_rq_truthful_mean': float(truthful['theta_rq'].mean()),\n",
    "    'theta_rq_false_mean': float(false['theta_rq'].mean()),\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('04_truthfulqa_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print('Results saved to 04_truthfulqa_results.csv and 04_truthfulqa_summary.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
