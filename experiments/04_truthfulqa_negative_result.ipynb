{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {
        "id": "cell-0"
      },
      "source": [
        "# TruthfulQA: Establishing SGI's Limitations\n",
        "\n",
        "This notebook demonstrates a critical **NEGATIVE result**: SGI cannot discriminate factual accuracy, only context engagement.\n",
        "\n",
        "## The Hypothesis\n",
        "\n",
        "TruthfulQA tests *misconceptions* — plausible-sounding false beliefs that are commonly held.\n",
        "\n",
        "**Example:**\n",
        "- Q: \"What causes the seasons?\"\n",
        "- Truthful: \"Axial tilt causes seasons\"\n",
        "- False: \"Distance from the sun causes seasons\"\n",
        "\n",
        "Both answers are **topically identical** — they discuss seasons, Earth, and causation. Angular distance cannot distinguish them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {
        "id": "cell-1"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-2",
      "metadata": {
        "id": "cell-2"
      },
      "outputs": [],
      "source": [
        "# Uncomment to install dependencies\n",
        "# !pip install -q datasets sentence-transformers numpy pandas matplotlib seaborn scipy scikit-learn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-3",
      "metadata": {
        "id": "cell-3"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy import stats\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "from sgi import (\n",
        "    compute_sgi,\n",
        "    load_truthfulqa,\n",
        "    print_dataset_summary,\n",
        "    compute_effect_size,\n",
        "    compute_cohens_d,\n",
        "    set_publication_style,\n",
        ")\n",
        "\n",
        "set_publication_style()\n",
        "print('Setup complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-4",
      "metadata": {
        "id": "cell-4"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-5",
      "metadata": {
        "id": "cell-5"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_NAME = 'all-mpnet-base-v2'\n",
        "MAX_SAMPLES = 2000\n",
        "\n",
        "print(f'Configuration:')\n",
        "print(f'  Model: {MODEL_NAME}')\n",
        "print(f'  MAX_SAMPLES: {MAX_SAMPLES}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-6",
      "metadata": {
        "id": "cell-6"
      },
      "source": [
        "## Load TruthfulQA Dataset\n",
        "\n",
        "TruthfulQA contains questions designed to elicit false beliefs. Unlike HaluEval, there is **no external context** — the \"context\" is essentially the same as the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-7",
      "metadata": {
        "id": "cell-7"
      },
      "outputs": [],
      "source": [
        "# Load TruthfulQA\n",
        "cases = load_truthfulqa(max_samples=MAX_SAMPLES)\n",
        "print_dataset_summary(cases, 'TruthfulQA')\n",
        "\n",
        "# Show examples\n",
        "print('\\n' + '='*70)\n",
        "print('EXAMPLE CASES')\n",
        "print('='*70)\n",
        "\n",
        "truthful_ex = next(c for c in cases if c.is_grounded)\n",
        "false_ex = next(c for c in cases if not c.is_grounded)\n",
        "\n",
        "print('\\nTRUTHFUL EXAMPLE:')\n",
        "print(f'  Question: {truthful_ex.question[:100]}...')\n",
        "print(f'  Response: {truthful_ex.response[:150]}...')\n",
        "\n",
        "print('\\nFALSE EXAMPLE:')\n",
        "print(f'  Question: {false_ex.question[:100]}...')\n",
        "print(f'  Response: {false_ex.response[:150]}...')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-8",
      "metadata": {
        "id": "cell-8"
      },
      "source": [
        "## Compute SGI for TruthfulQA\n",
        "\n",
        "We compute SGI using the standard formula:\n",
        "\n",
        "$$\\text{SGI} = \\frac{\\theta(r, q)}{\\theta(r, c)}$$\n",
        "\n",
        "**Key insight:** In TruthfulQA, `context ≈ question`, so the geometric relationship is fundamentally different from RAG scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-9",
      "metadata": {
        "id": "cell-9"
      },
      "outputs": [],
      "source": [
        "# Initialize encoder\n",
        "encoder = SentenceTransformer(MODEL_NAME)\n",
        "print(f'Model: {MODEL_NAME}')\n",
        "print(f'Embedding dimension: {encoder.get_sentence_embedding_dimension()}')\n",
        "\n",
        "# Compute SGI\n",
        "results = []\n",
        "\n",
        "for case in tqdm(cases, desc='Computing SGI for TruthfulQA'):\n",
        "    try:\n",
        "        q_emb = encoder.encode(case.question)\n",
        "        c_emb = encoder.encode(case.context)  # Note: context ≈ question in TruthfulQA\n",
        "        r_emb = encoder.encode(case.response)\n",
        "\n",
        "        sgi_result = compute_sgi(q_emb, c_emb, r_emb)\n",
        "\n",
        "        results.append({\n",
        "            'id': case.id,\n",
        "            'is_grounded': case.is_grounded,\n",
        "            'sgi': sgi_result.sgi,\n",
        "            'theta_rq': sgi_result.theta_rq,\n",
        "            'theta_rc': sgi_result.theta_rc,\n",
        "            'theta_qc': sgi_result.theta_qc,\n",
        "        })\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "del encoder\n",
        "gc.collect()\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "print(f'\\nProcessed: {len(df)} samples')\n",
        "print(f'  Truthful: {df[\"is_grounded\"].sum()}')\n",
        "print(f'  False: {(~df[\"is_grounded\"]).sum()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-10",
      "metadata": {
        "id": "cell-10"
      },
      "source": [
        "## Table 4: TruthfulQA Results\n",
        "\n",
        "Expected: AUC ≈ 0.478 (below chance), confirming SGI cannot discriminate factual accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-11",
      "metadata": {
        "id": "cell-11"
      },
      "outputs": [],
      "source": [
        "print('='*70)\n",
        "print('TABLE 4: TRUTHFULQA NEGATIVE RESULT')\n",
        "print('='*70)\n",
        "\n",
        "# Separate groups\n",
        "truthful = df[df['is_grounded']]\n",
        "false = df[~df['is_grounded']]\n",
        "\n",
        "# Basic statistics\n",
        "print(f'\\n{\"Metric\":<20} | {\"Truthful\":>12} | {\"False\":>12} | {\"Difference\":>12}')\n",
        "print('-'*65)\n",
        "\n",
        "for metric in ['sgi', 'theta_rq', 'theta_rc', 'theta_qc']:\n",
        "    t_mean = truthful[metric].mean()\n",
        "    f_mean = false[metric].mean()\n",
        "    diff = f_mean - t_mean\n",
        "    print(f'{metric:<20} | {t_mean:>12.3f} | {f_mean:>12.3f} | {diff:>+12.3f}')\n",
        "\n",
        "print('-'*65)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-12",
      "metadata": {
        "id": "cell-12"
      },
      "outputs": [],
      "source": [
        "# Effect size analysis\n",
        "print('\\n' + '='*70)\n",
        "print('EFFECT SIZE ANALYSIS')\n",
        "print('='*70)\n",
        "\n",
        "# Compute effect size for SGI\n",
        "sgi_result = compute_effect_size(\n",
        "    df['sgi'].values,\n",
        "    df['is_grounded'].values,\n",
        "    'SGI'\n",
        ")\n",
        "\n",
        "# Also compute for theta_rq (the key component)\n",
        "theta_rq_result = compute_effect_size(\n",
        "    df['theta_rq'].values,\n",
        "    df['is_grounded'].values,\n",
        "    'theta_rq'\n",
        ")\n",
        "\n",
        "print(f'\\nSGI Analysis:')\n",
        "print(f'  SGI (Truthful): {sgi_result.grounded_mean:.3f}')\n",
        "print(f'  SGI (False): {sgi_result.hallucinated_mean:.3f}')\n",
        "print(f\"  Cohen's d: {sgi_result.cohens_d:+.3f}\")\n",
        "print(f'  AUROC: {sgi_result.auroc:.3f}')\n",
        "print(f'  p-value: {sgi_result.p_value:.3f}')\n",
        "\n",
        "print(f'\\nθ(r,q) Analysis:')\n",
        "print(f'  θ(r,q) (Truthful): {theta_rq_result.grounded_mean:.3f}')\n",
        "print(f'  θ(r,q) (False): {theta_rq_result.hallucinated_mean:.3f}')\n",
        "print(f\"  Cohen's d: {theta_rq_result.cohens_d:+.3f}\")\n",
        "print(f'  AUROC: {theta_rq_result.auroc:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-13",
      "metadata": {
        "id": "cell-13"
      },
      "outputs": [],
      "source": [
        "# Paper comparison\n",
        "print('\\n' + '-'*70)\n",
        "print('COMPARISON WITH PAPER EXPECTED VALUES')\n",
        "print('-'*70)\n",
        "\n",
        "print(f'''\\n{\"Metric\":<25} | {\"Obtained\":>12} | {\"Paper Expected\":>15} | {\"Match?\":>8}''')\n",
        "print('-'*70)\n",
        "\n",
        "# Check if results are in expected range\n",
        "auc_match = 'YES' if 0.42 < sgi_result.auroc < 0.55 else 'NO'\n",
        "d_match = 'YES' if -0.3 < sgi_result.cohens_d < 0.1 else 'NO'\n",
        "\n",
        "print(f'{\"AUROC\":<25} | {sgi_result.auroc:>12.3f} | {\"~0.478\":>15} | {auc_match:>8}')\n",
        "print(f\"{\"Cohen's d\":<25} | {sgi_result.cohens_d:>+12.3f} | {\"~-0.14\":>15} | {d_match:>8}\")\n",
        "print('-'*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-14",
      "metadata": {
        "id": "cell-14"
      },
      "source": [
        "## Visualization: Why SGI Fails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-15",
      "metadata": {
        "id": "cell-15"
      },
      "outputs": [],
      "source": [
        "# Distribution comparison\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# SGI distributions\n",
        "ax = axes[0]\n",
        "ax.hist(truthful['sgi'], bins=30, alpha=0.6, label='Truthful', color='#2ecc71', density=True)\n",
        "ax.hist(false['sgi'], bins=30, alpha=0.6, label='False', color='#e74c3c', density=True)\n",
        "ax.axvline(truthful['sgi'].mean(), color='#27ae60', linestyle='--', linewidth=2)\n",
        "ax.axvline(false['sgi'].mean(), color='#c0392b', linestyle='--', linewidth=2)\n",
        "ax.set_xlabel('SGI')\n",
        "ax.set_ylabel('Density')\n",
        "ax.set_title(f\"SGI Distribution (d={sgi_result.cohens_d:.2f})\")\n",
        "ax.legend()\n",
        "\n",
        "# theta_rq distributions\n",
        "ax = axes[1]\n",
        "ax.hist(truthful['theta_rq'], bins=30, alpha=0.6, label='Truthful', color='#2ecc71', density=True)\n",
        "ax.hist(false['theta_rq'], bins=30, alpha=0.6, label='False', color='#e74c3c', density=True)\n",
        "ax.set_xlabel('θ(r,q)')\n",
        "ax.set_ylabel('Density')\n",
        "ax.set_title(f\"θ(r,q) Distribution\")\n",
        "ax.legend()\n",
        "\n",
        "# ROC curve\n",
        "ax = axes[2]\n",
        "y_true = (~df['is_grounded']).astype(int).values\n",
        "fpr, tpr, _ = roc_curve(y_true, df['sgi'].values)\n",
        "auroc = roc_auc_score(y_true, df['sgi'].values)\n",
        "\n",
        "ax.plot(fpr, tpr, color='#3498db', linewidth=2, label=f'SGI (AUC = {auroc:.3f})')\n",
        "ax.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random (AUC = 0.5)')\n",
        "ax.fill_between(fpr, tpr, alpha=0.2, color='#3498db')\n",
        "ax.set_xlabel('False Positive Rate')\n",
        "ax.set_ylabel('True Positive Rate')\n",
        "ax.set_title('ROC Curve (Below Chance!)')\n",
        "ax.legend(loc='lower right')\n",
        "\n",
        "fig.suptitle('TruthfulQA: SGI Cannot Discriminate Factual Accuracy', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('04_truthfulqa_negative_result.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-16",
      "metadata": {
        "id": "cell-16"
      },
      "source": [
        "## Why Does SGI Fail on TruthfulQA?\n",
        "\n",
        "### The Geometric Explanation\n",
        "\n",
        "SGI measures the ratio of angular distances:\n",
        "\n",
        "$$\\text{SGI} = \\frac{\\theta(r, q)}{\\theta(r, c)}$$\n",
        "\n",
        "This captures **topical engagement**: whether the response moves toward the context or stays near the question.\n",
        "\n",
        "### TruthfulQA's Structure\n",
        "\n",
        "In TruthfulQA:\n",
        "1. **No external context exists** — the \"context\" is essentially the question itself\n",
        "2. **Truthful and false answers are topically identical** — both discuss the same subject matter\n",
        "3. **The difference is semantic, not topical** — accuracy vs. misconception\n",
        "\n",
        "### What SGI Can and Cannot Detect\n",
        "\n",
        "| Scenario | Can SGI Detect? | Why? |\n",
        "|----------|-----------------|------|\n",
        "| RAG hallucination (topic drift) | YES | Response geometrically distant from context |\n",
        "| Factual misconception | NO | Response topically aligned, just semantically wrong |\n",
        "| Context disengagement | YES | Clear geometric signature |\n",
        "| Subtle factual errors | NO | No geometric signature |\n",
        "\n",
        "### Implications\n",
        "\n",
        "This negative result is **valuable** because it:\n",
        "1. Defines the operational boundaries of geometric methods\n",
        "2. Prevents misuse in factuality detection\n",
        "3. Motivates complementary approaches (e.g., knowledge retrieval, fact verification)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}