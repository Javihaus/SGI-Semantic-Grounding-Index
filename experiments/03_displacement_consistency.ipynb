{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYW78vO3UDL5"
      },
      "source": [
        "# Displacement Consistency: High-Performance Hallucination Detection\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBsXAZ5FUDL7"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bPwOa5eUDL7"
      },
      "outputs": [],
      "source": [
        "# Uncomment to install dependencies\n",
        "# !pip install -q datasets sentence-transformers spacy numpy pandas matplotlib seaborn scipy scikit-learn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hi0B54ieUDL8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, List, Dict, Tuple\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy import stats\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "from sgi import (\n",
        "    compute_sgi,\n",
        "    load_halueval_qa,\n",
        "    print_dataset_summary,\n",
        "    compute_effect_size,\n",
        "    compute_cohens_d,\n",
        "    set_publication_style,\n",
        ")\n",
        "\n",
        "set_publication_style()\n",
        "print('Setup complete.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQXuGvJnUDL8"
      },
      "outputs": [],
      "source": [
        "# Load spacy for sentence tokenization\n",
        "try:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "except:\n",
        "    import subprocess\n",
        "    subprocess.run(['python', '-m', 'spacy', 'download', 'en_core_web_sm'])\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "print('Spacy loaded.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpt6dNoyUDL9"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEtaD6S4UDL9"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_NAME = 'all-mpnet-base-v2'\n",
        "MAX_SAMPLES = 1000  # Reduce for faster execution; increase for paper reproduction\n",
        "\n",
        "print(f'Configuration:')\n",
        "print(f'  Embedding Model: {MODEL_NAME}')\n",
        "print(f'  MAX_SAMPLES: {MAX_SAMPLES}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwu___XrUDL9"
      },
      "source": [
        "## Displacement Metrics Dataclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EH7T553AUDL9"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DisplacementMetrics:\n",
        "    \"\"\"Context-anchored displacement metrics for hallucination detection.\"\"\"\n",
        "\n",
        "    # Primary metric: Displacement Consistency\n",
        "    displacement_consistency: float\n",
        "\n",
        "    # Expected Response Deviation\n",
        "    expected_response_deviation: float\n",
        "\n",
        "    # Displacement Magnitude\n",
        "    displacement_magnitude: float\n",
        "\n",
        "    # Tangent Space Residual\n",
        "    tangent_space_residual: float\n",
        "\n",
        "    # Local SGI (sentence-level)\n",
        "    local_sgi: float\n",
        "\n",
        "    # Additional context metrics\n",
        "    mean_claim_distance: float\n",
        "    max_claim_distance: float\n",
        "    trajectory_efficiency: float\n",
        "    num_claims: int\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        return self.__dict__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1n1ipYAUDL9"
      },
      "source": [
        "## Core Displacement Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yikGTbsUUDL9"
      },
      "outputs": [],
      "source": [
        "def extract_claims(text: str) -> List[str]:\n",
        "    \"\"\"Extract sentences as claims from text.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    claims = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 10]\n",
        "    return claims if claims else [text[:200]]\n",
        "\n",
        "\n",
        "def compute_displacement_consistency(context_emb: np.ndarray,\n",
        "                                     claim_embeddings: List[np.ndarray]) -> float:\n",
        "    \"\"\"\n",
        "    Compute displacement consistency from claim trajectories.\n",
        "\n",
        "    For grounded responses: consecutive claims maintain consistent\n",
        "    displacement direction from context.\n",
        "\n",
        "    For hallucinations: displacement directions are inconsistent,\n",
        "    wandering rather than progressing coherently.\n",
        "\n",
        "    Returns:\n",
        "        Mean correlation between consecutive displacement vectors.\n",
        "        High (near 1) = consistent/grounded, Low (near 0) = inconsistent/hallucinated\n",
        "    \"\"\"\n",
        "    if len(claim_embeddings) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    # Compute displacements from context to each claim\n",
        "    displacements = [claim - context_emb for claim in claim_embeddings]\n",
        "\n",
        "    # Compute correlations between consecutive displacements\n",
        "    correlations = []\n",
        "    for i in range(len(displacements) - 1):\n",
        "        d1 = displacements[i]\n",
        "        d2 = displacements[i + 1]\n",
        "\n",
        "        # Normalize\n",
        "        norm1 = np.linalg.norm(d1)\n",
        "        norm2 = np.linalg.norm(d2)\n",
        "\n",
        "        if norm1 > 1e-8 and norm2 > 1e-8:\n",
        "            d1_norm = d1 / norm1\n",
        "            d2_norm = d2 / norm2\n",
        "            correlations.append(np.dot(d1_norm, d2_norm))\n",
        "\n",
        "    return np.mean(correlations) if correlations else 0.0\n",
        "\n",
        "\n",
        "def compute_expected_response_deviation(question_emb: np.ndarray,\n",
        "                                        context_emb: np.ndarray,\n",
        "                                        response_emb: np.ndarray,\n",
        "                                        reference_pairs: List[Tuple[np.ndarray, np.ndarray]]) -> float:\n",
        "    \"\"\"\n",
        "    Compute deviation from expected response location.\n",
        "\n",
        "    Uses k-NN of similar questions to predict where the response should be.\n",
        "    Hallucinations deviate significantly from this prediction.\n",
        "    \"\"\"\n",
        "    if not reference_pairs:\n",
        "        return 0.0\n",
        "\n",
        "    # Compute expected displacement based on reference pairs\n",
        "    mean_displacement = np.mean([r - q for q, r in reference_pairs], axis=0)\n",
        "\n",
        "    # Predicted response location\n",
        "    predicted_response = question_emb + mean_displacement\n",
        "\n",
        "    # Deviation from prediction\n",
        "    deviation = np.linalg.norm(response_emb - predicted_response)\n",
        "\n",
        "    return deviation\n",
        "\n",
        "\n",
        "def compute_tangent_space_residual(context_emb: np.ndarray,\n",
        "                                   response_emb: np.ndarray,\n",
        "                                   reference_responses: List[np.ndarray]) -> float:\n",
        "    \"\"\"\n",
        "    Compute residual after projection to local tangent space.\n",
        "\n",
        "    Grounded responses lie close to the tangent space defined by\n",
        "    similar grounded responses.\n",
        "    \"\"\"\n",
        "    if len(reference_responses) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    # Build local basis from reference displacements\n",
        "    displacements = [r - context_emb for r in reference_responses]\n",
        "\n",
        "    # Stack and compute SVD for principal directions\n",
        "    D = np.vstack(displacements)\n",
        "    try:\n",
        "        U, S, Vt = np.linalg.svd(D, full_matrices=False)\n",
        "        # Use top-k principal components\n",
        "        k = min(3, len(S))\n",
        "        basis = Vt[:k].T\n",
        "\n",
        "        # Project response displacement onto basis\n",
        "        response_disp = response_emb - context_emb\n",
        "        projection = basis @ (basis.T @ response_disp)\n",
        "\n",
        "        # Residual is the part not explained by the basis\n",
        "        residual = np.linalg.norm(response_disp - projection)\n",
        "        return residual\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def compute_local_sgi(question_emb: np.ndarray,\n",
        "                      context_emb: np.ndarray,\n",
        "                      claim_embeddings: List[np.ndarray]) -> float:\n",
        "    \"\"\"\n",
        "    Compute sentence-level SGI (Local SGI / LSGI).\n",
        "\n",
        "    Aggregates SGI scores across individual claims.\n",
        "    \"\"\"\n",
        "    if not claim_embeddings:\n",
        "        return 1.0\n",
        "\n",
        "    sgi_scores = []\n",
        "    for claim_emb in claim_embeddings:\n",
        "        result = compute_sgi(question_emb, context_emb, claim_emb)\n",
        "        sgi_scores.append(result.sgi)\n",
        "\n",
        "    return np.mean(sgi_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dUUlGcdUDL-"
      },
      "source": [
        "## Compute All Displacement Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo-VF2OAUDL-"
      },
      "outputs": [],
      "source": [
        "def compute_displacement_metrics(question: str,\n",
        "                                  context: str,\n",
        "                                  response: str,\n",
        "                                  encoder: SentenceTransformer,\n",
        "                                  reference_pairs: Optional[List[Tuple[np.ndarray, np.ndarray]]] = None,\n",
        "                                  reference_responses: Optional[List[np.ndarray]] = None) -> Optional[DisplacementMetrics]:\n",
        "    \"\"\"\n",
        "    Compute all displacement-based metrics for a single case.\n",
        "    \"\"\"\n",
        "    # Extract claims from response\n",
        "    claims = extract_claims(response)\n",
        "    if not claims:\n",
        "        return None\n",
        "\n",
        "    # Encode all texts\n",
        "    question_emb = encoder.encode(question)\n",
        "    context_emb = encoder.encode(context)\n",
        "    response_emb = encoder.encode(response)\n",
        "    claim_embeddings = [encoder.encode(c) for c in claims]\n",
        "\n",
        "    # 1. Displacement Consistency (DC)\n",
        "    dc = compute_displacement_consistency(context_emb, claim_embeddings)\n",
        "\n",
        "    # 2. Expected Response Deviation (ERD)\n",
        "    erd = compute_expected_response_deviation(\n",
        "        question_emb, context_emb, response_emb,\n",
        "        reference_pairs or []\n",
        "    )\n",
        "\n",
        "    # 3. Displacement Magnitude\n",
        "    displacement_magnitude = np.linalg.norm(response_emb - context_emb)\n",
        "\n",
        "    # 4. Tangent Space Residual (TSR)\n",
        "    tsr = compute_tangent_space_residual(\n",
        "        context_emb, response_emb,\n",
        "        reference_responses or []\n",
        "    )\n",
        "\n",
        "    # 5. Local SGI (LSGI)\n",
        "    lsgi = compute_local_sgi(question_emb, context_emb, claim_embeddings)\n",
        "\n",
        "    # Additional metrics\n",
        "    claim_distances = [np.linalg.norm(c - context_emb) for c in claim_embeddings]\n",
        "\n",
        "    # Trajectory efficiency (direct distance / path length)\n",
        "    trajectory = [context_emb] + claim_embeddings\n",
        "    path_length = sum(np.linalg.norm(trajectory[i+1] - trajectory[i])\n",
        "                      for i in range(len(trajectory) - 1))\n",
        "    direct_distance = np.linalg.norm(claim_embeddings[-1] - context_emb) if claim_embeddings else 0\n",
        "    efficiency = direct_distance / (path_length + 1e-8)\n",
        "\n",
        "    return DisplacementMetrics(\n",
        "        displacement_consistency=dc,\n",
        "        expected_response_deviation=erd,\n",
        "        displacement_magnitude=displacement_magnitude,\n",
        "        tangent_space_residual=tsr,\n",
        "        local_sgi=lsgi,\n",
        "        mean_claim_distance=np.mean(claim_distances),\n",
        "        max_claim_distance=np.max(claim_distances),\n",
        "        trajectory_efficiency=efficiency,\n",
        "        num_claims=len(claims)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XADXipxFUDL-"
      },
      "source": [
        "## Load Data and Run Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s76bmxklUDL-"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "cases = load_halueval_qa(max_samples=MAX_SAMPLES)\n",
        "print_dataset_summary(cases, 'HaluEval QA')\n",
        "\n",
        "# Initialize encoder\n",
        "encoder = SentenceTransformer(MODEL_NAME)\n",
        "print(f'\\nEncoder loaded: {MODEL_NAME}')\n",
        "print(f'Embedding dimension: {encoder.get_sentence_embedding_dimension()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhLJ7N7pUDL-"
      },
      "outputs": [],
      "source": [
        "# Build reference set from grounded samples (for ERD and TSR)\n",
        "print('Building reference set from grounded samples...')\n",
        "grounded_cases = [c for c in cases if c.is_grounded][:200]\n",
        "\n",
        "reference_pairs = []\n",
        "reference_responses = []\n",
        "\n",
        "for case in tqdm(grounded_cases, desc='Building references'):\n",
        "    try:\n",
        "        q_emb = encoder.encode(case.question)\n",
        "        r_emb = encoder.encode(case.response)\n",
        "        reference_pairs.append((q_emb, r_emb))\n",
        "        reference_responses.append(r_emb)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "print(f'Reference set: {len(reference_pairs)} pairs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KH2pgnzKUDL-"
      },
      "outputs": [],
      "source": [
        "# Compute displacement metrics for all cases\n",
        "results = []\n",
        "\n",
        "for case in tqdm(cases, desc='Computing displacement metrics'):\n",
        "    try:\n",
        "        metrics = compute_displacement_metrics(\n",
        "            question=case.question,\n",
        "            context=case.context,\n",
        "            response=case.response,\n",
        "            encoder=encoder,\n",
        "            reference_pairs=reference_pairs,\n",
        "            reference_responses=reference_responses\n",
        "        )\n",
        "\n",
        "        if metrics is None:\n",
        "            continue\n",
        "\n",
        "        result = metrics.to_dict()\n",
        "        result['id'] = case.id\n",
        "        result['is_grounded'] = case.is_grounded\n",
        "        results.append(result)\n",
        "\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "print(f'\\nProcessed: {len(df)} samples')\n",
        "print(f'  Grounded: {df[\"is_grounded\"].sum()}')\n",
        "print(f'  Hallucinated: {(~df[\"is_grounded\"]).sum()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTkyfEyyUDL_"
      },
      "source": [
        "## Evaluate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gJmG-vrUDL_"
      },
      "outputs": [],
      "source": [
        "# Define metrics to evaluate\n",
        "METRICS = [\n",
        "    ('displacement_consistency', False),  # Higher = more grounded (invert for AUROC)\n",
        "    ('expected_response_deviation', True),  # Higher = more hallucinated\n",
        "    ('displacement_magnitude', True),  # Higher = more hallucinated\n",
        "    ('tangent_space_residual', True),  # Higher = more hallucinated\n",
        "    ('local_sgi', True),  # Higher = more hallucinated\n",
        "]\n",
        "\n",
        "print('='*80)\n",
        "print('DISPLACEMENT METRICS EVALUATION')\n",
        "print('='*80)\n",
        "print(f'\\n{\"Method\":<30} | {\"Grounded\":>10} | {\"Halluc\":>10} | {\"Cohen\\'s d\":>10} | {\"AUROC\":>8}')\n",
        "print('-'*80)\n",
        "\n",
        "metric_results = []\n",
        "\n",
        "for metric_name, higher_is_hallucinated in METRICS:\n",
        "    values = df[metric_name].values\n",
        "    labels = df['is_grounded'].values\n",
        "\n",
        "    result = compute_effect_size(values, labels, metric_name,\n",
        "                                 positive_class_is_hallucinated=higher_is_hallucinated)\n",
        "\n",
        "    print(f'{metric_name:<30} | {result.grounded_mean:>10.4f} | {result.hallucinated_mean:>10.4f} | '\n",
        "          f'{result.cohens_d:>+10.3f} | {result.auroc:>8.4f}')\n",
        "\n",
        "    metric_results.append({\n",
        "        'Method': metric_name,\n",
        "        'Grounded Mean': result.grounded_mean,\n",
        "        'Hallucinated Mean': result.hallucinated_mean,\n",
        "        \"Cohen's d\": result.cohens_d,\n",
        "        'AUROC': result.auroc,\n",
        "        'p-value': result.p_value,\n",
        "    })\n",
        "\n",
        "print('-'*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaS3UialUDL_"
      },
      "source": [
        "## Comparison with Pre-computed Results\n",
        "\n",
        "Load and display the pre-computed results from the `results/` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3S5wAIPUDL_"
      },
      "outputs": [],
      "source": [
        "# Load pre-computed results\n",
        "import os\n",
        "\n",
        "precomputed_results = []\n",
        "results_dir = '../results'\n",
        "\n",
        "if os.path.exists(results_dir):\n",
        "    for exp_dir in os.listdir(results_dir):\n",
        "        csv_files = [f for f in os.listdir(os.path.join(results_dir, exp_dir))\n",
        "                     if f.endswith('.csv') and 'geometric_methods_summary' in f]\n",
        "        for csv_file in csv_files:\n",
        "            try:\n",
        "                df_precomputed = pd.read_csv(os.path.join(results_dir, exp_dir, csv_file))\n",
        "                df_precomputed['Source'] = exp_dir\n",
        "                precomputed_results.append(df_precomputed)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "if precomputed_results:\n",
        "    print('='*80)\n",
        "    print('PRE-COMPUTED RESULTS FROM results/ DIRECTORY')\n",
        "    print('='*80)\n",
        "\n",
        "    combined_precomputed = pd.concat(precomputed_results, ignore_index=True)\n",
        "\n",
        "    # Aggregate by method\n",
        "    summary = combined_precomputed.groupby('Method').agg({\n",
        "        'Cross-Domain AUROC': ['mean', 'std'],\n",
        "        'Within-Domain AUROC': ['mean', 'std']\n",
        "    }).round(4)\n",
        "\n",
        "    print('\\nMethod Performance Summary (across embedding models):')\n",
        "    print(summary.to_string())\n",
        "else:\n",
        "    print('No pre-computed results found in results/ directory.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AECzVgxkUDL_"
      },
      "source": [
        "## Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_kmAH0uUDL_"
      },
      "outputs": [],
      "source": [
        "# ROC curves for all metrics\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "colors = plt.cm.Set2(np.linspace(0, 1, len(METRICS)))\n",
        "\n",
        "for idx, ((metric_name, higher_is_hallucinated), color) in enumerate(zip(METRICS, colors)):\n",
        "    ax = axes[idx]\n",
        "\n",
        "    values = df[metric_name].values\n",
        "    y_true = (~df['is_grounded']).astype(int).values\n",
        "\n",
        "    if not higher_is_hallucinated:\n",
        "        values = -values\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true, values)\n",
        "    auroc = roc_auc_score(y_true, values)\n",
        "\n",
        "    ax.plot(fpr, tpr, color=color, linewidth=2, label=f'AUC = {auroc:.4f}')\n",
        "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
        "    ax.set_xlabel('False Positive Rate')\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.set_title(metric_name.replace('_', ' ').title())\n",
        "    ax.legend(loc='lower right')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Hide unused subplot\n",
        "axes[-1].axis('off')\n",
        "\n",
        "fig.suptitle('ROC Curves for Displacement Metrics', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('displacement_roc_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVEdOicgUDL_"
      },
      "outputs": [],
      "source": [
        "# Distribution comparison\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, (metric_name, _) in enumerate(METRICS):\n",
        "    ax = axes[idx]\n",
        "\n",
        "    grounded = df[df['is_grounded']][metric_name]\n",
        "    hallucinated = df[~df['is_grounded']][metric_name]\n",
        "\n",
        "    ax.hist(grounded, bins=30, alpha=0.6, label='Grounded', color='#2ecc71', density=True)\n",
        "    ax.hist(hallucinated, bins=30, alpha=0.6, label='Hallucinated', color='#e74c3c', density=True)\n",
        "\n",
        "    ax.set_xlabel(metric_name.replace('_', ' ').title())\n",
        "    ax.set_ylabel('Density')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "axes[-1].axis('off')\n",
        "\n",
        "fig.suptitle('Distribution of Displacement Metrics', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('displacement_distributions.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}