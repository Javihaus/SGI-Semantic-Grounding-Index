{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZzGVOfSSxd4"
      },
      "source": [
        "# Cross-Model Validation of the Semantic Grounding Index\n",
        "\n",
        "**Research Question:** Is SGI a property of the text itself, or an artifact of specific embedding geometries?\n",
        "\n",
        "If SGI captures something fundamental about how language models handle uncertainty, the signal should be:\n",
        "1. **Consistent across embedding architectures** — high correlation between SGI scores from different encoders\n",
        "2. **Robust in effect size** — Cohen's d should remain significant across embedding spaces\n",
        "3. **Stable in ranking** — samples ranked as \"high risk\" by one model should be ranked similarly by others\n",
        "\n",
        "If the signal is an artifact of a particular embedding geometry, we'd expect low correlation, inconsistent effect sizes, and different samples flagged by different models.\n",
        "\n",
        "---\n",
        "\n",
        "**Models tested:**\n",
        "\n",
        "| Model | Dimension | Training Regime |\n",
        "|-------|-----------|----------------|\n",
        "| `all-mpnet-base-v2` | 768 | General-purpose sentence embeddings |\n",
        "| `all-MiniLM-L6-v2` | 384 | Distilled, smaller architecture |\n",
        "| `bge-base-en-v1.5` | 768 | BAAI contrastive learning |\n",
        "| `e5-base-v2` | 768 | Microsoft instruction-tuned |\n",
        "| `gte-base` | 768 | Alibaba general text embeddings |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnwganxiSxd6"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dEayVFMSxd6"
      },
      "outputs": [],
      "source": [
        "# Uncomment to install dependencies\n",
        "# !pip install -q datasets sentence-transformers spacy numpy pandas matplotlib seaborn scipy scikit-learn\n",
        "# !python -m spacy download en_core_web_sm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E98yY3K3Sxd7"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Local imports\n",
        "from sgi import (\n",
        "    compute_sgi,\n",
        "    load_halueval_qa,\n",
        "    print_dataset_summary,\n",
        "    compute_effect_size,\n",
        "    compute_correlation_matrix,\n",
        "    compute_pairwise_correlations,\n",
        "    compute_topk_overlap_matrix,\n",
        "    set_publication_style,\n",
        "    create_summary_figure,\n",
        ")\n",
        "\n",
        "set_publication_style()\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "print('Setup complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S-KOY_GSxd7"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9v67ayfSxd7"
      },
      "outputs": [],
      "source": [
        "# Embedding models to validate\n",
        "EMBEDDING_MODELS = {\n",
        "    'mpnet': 'all-mpnet-base-v2',\n",
        "    'minilm': 'all-MiniLM-L6-v2',\n",
        "    'bge': 'BAAI/bge-base-en-v1.5',\n",
        "    'e5': 'intfloat/e5-base-v2',\n",
        "    'gte': 'thenlper/gte-base',\n",
        "}\n",
        "\n",
        "# Sample size (paper uses n=5,000 for reproducibility)\n",
        "MAX_SAMPLES = 5000\n",
        "\n",
        "print(f'Will validate SGI across {len(EMBEDDING_MODELS)} embedding models:')\n",
        "for name, model_id in EMBEDDING_MODELS.items():\n",
        "    print(f'  - {name}: {model_id}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DGNibQkSxd7"
      },
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIuPBvmgSxd7"
      },
      "outputs": [],
      "source": [
        "cases = load_halueval_qa(max_samples=MAX_SAMPLES)\n",
        "print_dataset_summary(cases, 'HaluEval QA')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXGlTKpkSxd8"
      },
      "source": [
        "## 2. Compute SGI Across Models\n",
        "\n",
        "For each embedding model, we compute:\n",
        "\n",
        "$$\\text{SGI} = \\frac{\\theta(r, q)}{\\theta(r, c)}$$\n",
        "\n",
        "where $\\theta$ is the angular distance and $r$, $q$, $c$ are the L2-normalized embeddings of response, question, and context respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKj6wm6eSxd8"
      },
      "outputs": [],
      "source": [
        "def extract_first_claim(text: str) -> str:\n",
        "    \"\"\"Extract first sentence as primary claim.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    for sent in doc.sents:\n",
        "        if len(sent.text.strip()) > 5:\n",
        "            return sent.text.strip()\n",
        "    return text[:200]\n",
        "\n",
        "\n",
        "def compute_sgi_for_model(cases, model_name: str, model_id: str) -> pd.DataFrame:\n",
        "    \"\"\"Compute SGI for all cases using specified embedding model.\"\"\"\n",
        "    print(f'\\n{\"=\"*60}')\n",
        "    print(f'Model: {model_name} ({model_id})')\n",
        "    print(f'{\"=\"*60}')\n",
        "\n",
        "    encoder = SentenceTransformer(model_id)\n",
        "    print(f'Embedding dimension: {encoder.get_sentence_embedding_dimension()}')\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for case in tqdm(cases, desc=f'Computing SGI ({model_name})'):\n",
        "        try:\n",
        "            q_emb = encoder.encode(case.question)\n",
        "            c_emb = encoder.encode(case.context)\n",
        "            r_emb = encoder.encode(extract_first_claim(case.response))\n",
        "\n",
        "            sgi_result = compute_sgi(q_emb, c_emb, r_emb)\n",
        "\n",
        "            results.append({\n",
        "                'id': case.id,\n",
        "                'is_grounded': case.is_grounded,\n",
        "                f'sgi_{model_name}': sgi_result.sgi,\n",
        "                f'theta_rq_{model_name}': sgi_result.theta_rq,\n",
        "                f'theta_rc_{model_name}': sgi_result.theta_rc,\n",
        "            })\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    # Clean up\n",
        "    del encoder\n",
        "    gc.collect()\n",
        "\n",
        "    print(f'Processed: {len(results)} samples')\n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHJ-ZQwiSxd8"
      },
      "outputs": [],
      "source": [
        "# Run analysis for each model\n",
        "model_dfs = {}\n",
        "\n",
        "for model_name, model_id in EMBEDDING_MODELS.items():\n",
        "    model_dfs[model_name] = compute_sgi_for_model(cases, model_name, model_id)\n",
        "\n",
        "print(f'\\n\\nCompleted analysis for {len(model_dfs)} models.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RERIU-VOSxd8"
      },
      "outputs": [],
      "source": [
        "# Merge results into single dataframe\n",
        "model_names = list(model_dfs.keys())\n",
        "merged_df = model_dfs[model_names[0]][['id', 'is_grounded']].copy()\n",
        "\n",
        "for model_name, df in model_dfs.items():\n",
        "    cols_to_merge = ['id'] + [c for c in df.columns if model_name in c]\n",
        "    merged_df = merged_df.merge(df[cols_to_merge], on='id', how='inner')\n",
        "\n",
        "print(f'Merged: {len(merged_df)} samples with SGI from {len(model_names)} models')\n",
        "print(f'  Grounded: {merged_df[\"is_grounded\"].sum()}')\n",
        "print(f'  Hallucinated: {(~merged_df[\"is_grounded\"]).sum()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u-Vq0dBSxd8"
      },
      "source": [
        "## 3. Cross-Model Validation Tests\n",
        "\n",
        "### Test 1: Effect Size Consistency\n",
        "\n",
        "Does each embedding model show significant separation between grounded and hallucinated responses?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AwJx3HWSxd9"
      },
      "outputs": [],
      "source": [
        "print('='*70)\n",
        "print('TEST 1: EFFECT SIZE CONSISTENCY')\n",
        "print('='*70)\n",
        "print(f'\\n{\"Model\":<10} | {\"Grounded\":>10} | {\"Halluc\":>10} | {\"Cohen\\'s d\":>10} | {\"AUROC\":>8} | {\"p-value\":>12}')\n",
        "print('-'*70)\n",
        "\n",
        "effect_sizes = {}\n",
        "\n",
        "for model_name in model_names:\n",
        "    sgi_col = f'sgi_{model_name}'\n",
        "    values = merged_df[sgi_col].values\n",
        "    labels = merged_df['is_grounded'].values\n",
        "\n",
        "    result = compute_effect_size(values, labels, model_name)\n",
        "    effect_sizes[model_name] = result.to_dict()\n",
        "\n",
        "    sig = '***' if result.p_value < 0.001 else '**' if result.p_value < 0.01 else '*' if result.p_value < 0.05 else ''\n",
        "    print(f'{model_name:<10} | {result.grounded_mean:>10.4f} | {result.hallucinated_mean:>10.4f} | '\n",
        "          f'{result.cohens_d:>+10.3f} | {result.auroc:>8.3f} | {result.p_value:>10.2e} {sig}')\n",
        "\n",
        "print('-'*70)\n",
        "\n",
        "# Summary\n",
        "d_values = [r['cohens_d'] for r in effect_sizes.values()]\n",
        "auc_values = [r['auroc'] for r in effect_sizes.values()]\n",
        "n_significant = sum(1 for r in effect_sizes.values() if r['significant'])\n",
        "\n",
        "print(f'\\nSignificant effects (p<0.05): {n_significant}/{len(model_names)}')\n",
        "print(f'Cohen\\'s d: mean={np.mean(d_values):.3f}, range=[{min(d_values):.3f}, {max(d_values):.3f}]')\n",
        "print(f'AUROC: mean={np.mean(auc_values):.3f}, range=[{min(auc_values):.3f}, {max(auc_values):.3f}]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_MFKOCMSxd9"
      },
      "source": [
        "### Test 2: Cross-Model Correlation (Pearson)\n",
        "\n",
        "If SGI captures something fundamental, scores from different models should correlate strongly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NAB8gwtSxd9"
      },
      "outputs": [],
      "source": [
        "print('='*70)\n",
        "print('TEST 2: CROSS-MODEL CORRELATION (PEARSON)')\n",
        "print('='*70)\n",
        "\n",
        "sgi_cols = [f'sgi_{m}' for m in model_names]\n",
        "sgi_df = merged_df[sgi_cols].copy()\n",
        "sgi_df.columns = model_names\n",
        "\n",
        "pearson_matrix = compute_correlation_matrix(sgi_df, model_names, method='pearson')\n",
        "pearson_stats = compute_pairwise_correlations(pearson_matrix)\n",
        "\n",
        "print('\\nPearson Correlation Matrix:')\n",
        "print(pearson_matrix.round(3).to_string())\n",
        "print(f'\\nOff-diagonal: mean={pearson_stats[\"mean\"]:.3f}, range=[{pearson_stats[\"min\"]:.3f}, {pearson_stats[\"max\"]:.3f}]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcB3sX1qSxd9"
      },
      "source": [
        "### Test 3: Ranking Agreement (Spearman)\n",
        "\n",
        "Do models agree on which samples are most/least grounded?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3ndEgopSxd9"
      },
      "outputs": [],
      "source": [
        "print('='*70)\n",
        "print('TEST 3: RANKING AGREEMENT (SPEARMAN)')\n",
        "print('='*70)\n",
        "\n",
        "spearman_matrix = compute_correlation_matrix(sgi_df, model_names, method='spearman')\n",
        "spearman_stats = compute_pairwise_correlations(spearman_matrix)\n",
        "\n",
        "print('\\nSpearman Rank Correlation Matrix:')\n",
        "print(spearman_matrix.round(3).to_string())\n",
        "print(f'\\nOff-diagonal: mean={spearman_stats[\"mean\"]:.3f}, range=[{spearman_stats[\"min\"]:.3f}, {spearman_stats[\"max\"]:.3f}]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9G2a7wvSxd9"
      },
      "source": [
        "### Test 4: Top-K Flagging Agreement\n",
        "\n",
        "If we flag the top 10% as \"high risk\" by each model, how much overlap is there?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2EzFutHSxd9"
      },
      "outputs": [],
      "source": [
        "print('='*70)\n",
        "print('TEST 4: TOP-K FLAGGING AGREEMENT')\n",
        "print('='*70)\n",
        "\n",
        "for k_pct in [10, 20, 30]:\n",
        "    overlap_matrix = compute_topk_overlap_matrix(merged_df, sgi_cols, k_percent=k_pct)\n",
        "    overlap_matrix.index = model_names\n",
        "    overlap_matrix.columns = model_names\n",
        "\n",
        "    # Mean off-diagonal Jaccard\n",
        "    off_diag = [overlap_matrix.iloc[i, j] for i in range(len(model_names))\n",
        "                for j in range(i+1, len(model_names))]\n",
        "\n",
        "    print(f'\\nTop-{k_pct}% overlap (Jaccard similarity):')\n",
        "    print(overlap_matrix.round(3).to_string())\n",
        "    print(f'Mean pairwise Jaccard: {np.mean(off_diag):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi7YlE07Sxd9"
      },
      "source": [
        "### Test 5: Component Analysis\n",
        "\n",
        "Which component of SGI drives the signal — response-question proximity or response-context distance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E06AjN3GSxd-"
      },
      "outputs": [],
      "source": [
        "print('='*70)\n",
        "print('TEST 5: COMPONENT ANALYSIS')\n",
        "print('='*70)\n",
        "print(f'\\n{\"Model\":<10} | {\"d(θ_rq)\":>10} | {\"d(θ_rc)\":>10} | Driver')\n",
        "print('-'*50)\n",
        "\n",
        "for model_name in model_names:\n",
        "    theta_rq_col = f'theta_rq_{model_name}'\n",
        "    theta_rc_col = f'theta_rc_{model_name}'\n",
        "    labels = merged_df['is_grounded'].values\n",
        "\n",
        "    d_rq = compute_effect_size(merged_df[theta_rq_col].values, labels, 'θ_rq')\n",
        "    d_rc = compute_effect_size(merged_df[theta_rc_col].values, labels, 'θ_rc')\n",
        "\n",
        "    driver = 'θ(r,q)' if abs(d_rq.cohens_d) > abs(d_rc.cohens_d) else 'θ(r,c)'\n",
        "    print(f'{model_name:<10} | {d_rq.cohens_d:>+10.3f} | {d_rc.cohens_d:>+10.3f} | {driver}')\n",
        "\n",
        "print('-'*50)\n",
        "print('\\nInterpretation:')\n",
        "print('  • Negative d(θ_rq): Hallucinations are CLOSER to question')\n",
        "print('  • Positive d(θ_rc): Hallucinations are FARTHER from context')\n",
        "print('  • Both effects together = \"semantic laziness\" signature')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEFlubc5Sxd-"
      },
      "source": [
        "## 4. Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmJfYuZZSxd-"
      },
      "outputs": [],
      "source": [
        "fig = create_summary_figure(\n",
        "    df=merged_df,\n",
        "    effect_sizes=effect_sizes,\n",
        "    pearson_matrix=pearson_matrix,\n",
        "    spearman_matrix=spearman_matrix,\n",
        "    model_names=model_names,\n",
        "    save_path='cross_model_validation.png',\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5UL2-NwSxd-"
      },
      "source": [
        "## 5. Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cuY7E26Sxd-"
      },
      "outputs": [],
      "source": [
        "from sgi.analysis import summarize_cross_model_validation\n",
        "\n",
        "summary = summarize_cross_model_validation(\n",
        "    effect_sizes={m: type('obj', (object,), effect_sizes[m])() for m in model_names},\n",
        "    pearson_corr=pearson_stats,\n",
        "    spearman_corr=spearman_stats,\n",
        ")\n",
        "print(summary)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}