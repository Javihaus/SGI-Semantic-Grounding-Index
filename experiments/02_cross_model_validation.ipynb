{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Model Validation of the Semantic Grounding Index\n",
    "\n",
    "**Research Question:** Is SGI a property of the text itself, or an artifact of specific embedding geometries?\n",
    "\n",
    "If SGI captures something fundamental about how language models handle uncertainty, the signal should be:\n",
    "1. **Consistent across embedding architectures** — high correlation between SGI scores from different encoders\n",
    "2. **Robust in effect size** — Cohen's d should remain significant across embedding spaces\n",
    "3. **Stable in ranking** — samples ranked as \"high risk\" by one model should be ranked similarly by others\n",
    "\n",
    "If the signal is an artifact of a particular embedding geometry, we'd expect low correlation, inconsistent effect sizes, and different samples flagged by different models.\n",
    "\n",
    "---\n",
    "\n",
    "**Models tested:**\n",
    "\n",
    "| Model | Dimension | Training Regime |\n",
    "|-------|-----------|----------------|\n",
    "| `all-mpnet-base-v2` | 768 | General-purpose sentence embeddings |\n",
    "| `all-MiniLM-L6-v2` | 384 | Distilled, smaller architecture |\n",
    "| `bge-base-en-v1.5` | 768 | BAAI contrastive learning |\n",
    "| `e5-base-v2` | 768 | Microsoft instruction-tuned |\n",
    "| `gte-base` | 768 | Alibaba general text embeddings |\n",
    "\n",
    "---\n",
    "\n",
    "**Reference:** Marín (2024) \"Semantic Grounding Index\" [arXiv:2512.13771](https://arxiv.org/abs/2512.13771)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install dependencies\n",
    "# !pip install -q datasets sentence-transformers spacy numpy pandas matplotlib seaborn scipy scikit-learn\n",
    "# !python -m spacy download en_core_web_sm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Local imports\n",
    "from sgi import (\n",
    "    compute_sgi,\n",
    "    load_halueval_qa,\n",
    "    print_dataset_summary,\n",
    "    compute_effect_size,\n",
    "    compute_correlation_matrix,\n",
    "    compute_pairwise_correlations,\n",
    "    compute_topk_overlap_matrix,\n",
    "    set_publication_style,\n",
    "    create_summary_figure,\n",
    ")\n",
    "\n",
    "set_publication_style()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Embedding models to validate\nEMBEDDING_MODELS = {\n    'mpnet': 'all-mpnet-base-v2',\n    'minilm': 'all-MiniLM-L6-v2',\n    'bge': 'BAAI/bge-base-en-v1.5',\n    'e5': 'intfloat/e5-base-v2',\n    'gte': 'thenlper/gte-base',\n}\n\n# Sample size (paper uses n=5,000 for reproducibility)\nMAX_SAMPLES = 5000\n\nprint(f'Will validate SGI across {len(EMBEDDING_MODELS)} embedding models:')\nfor name, model_id in EMBEDDING_MODELS.items():\n    print(f'  - {name}: {model_id}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = load_halueval_qa(max_samples=MAX_SAMPLES)\n",
    "print_dataset_summary(cases, 'HaluEval QA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute SGI Across Models\n",
    "\n",
    "For each embedding model, we compute:\n",
    "\n",
    "$$\\text{SGI} = \\frac{\\theta(r, q)}{\\theta(r, c)}$$\n",
    "\n",
    "where $\\theta$ is the angular distance and $r$, $q$, $c$ are the L2-normalized embeddings of response, question, and context respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_claim(text: str) -> str:\n",
    "    \"\"\"Extract first sentence as primary claim.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        if len(sent.text.strip()) > 5:\n",
    "            return sent.text.strip()\n",
    "    return text[:200]\n",
    "\n",
    "\n",
    "def compute_sgi_for_model(cases, model_name: str, model_id: str) -> pd.DataFrame:\n",
    "    \"\"\"Compute SGI for all cases using specified embedding model.\"\"\"\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Model: {model_name} ({model_id})')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    encoder = SentenceTransformer(model_id)\n",
    "    print(f'Embedding dimension: {encoder.get_sentence_embedding_dimension()}')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for case in tqdm(cases, desc=f'Computing SGI ({model_name})'):\n",
    "        try:\n",
    "            q_emb = encoder.encode(case.question)\n",
    "            c_emb = encoder.encode(case.context)\n",
    "            r_emb = encoder.encode(extract_first_claim(case.response))\n",
    "            \n",
    "            sgi_result = compute_sgi(q_emb, c_emb, r_emb)\n",
    "            \n",
    "            results.append({\n",
    "                'id': case.id,\n",
    "                'is_grounded': case.is_grounded,\n",
    "                f'sgi_{model_name}': sgi_result.sgi,\n",
    "                f'theta_rq_{model_name}': sgi_result.theta_rq,\n",
    "                f'theta_rc_{model_name}': sgi_result.theta_rc,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # Clean up\n",
    "    del encoder\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f'Processed: {len(results)} samples')\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis for each model\n",
    "model_dfs = {}\n",
    "\n",
    "for model_name, model_id in EMBEDDING_MODELS.items():\n",
    "    model_dfs[model_name] = compute_sgi_for_model(cases, model_name, model_id)\n",
    "\n",
    "print(f'\\n\\nCompleted analysis for {len(model_dfs)} models.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge results into single dataframe\n",
    "model_names = list(model_dfs.keys())\n",
    "merged_df = model_dfs[model_names[0]][['id', 'is_grounded']].copy()\n",
    "\n",
    "for model_name, df in model_dfs.items():\n",
    "    cols_to_merge = ['id'] + [c for c in df.columns if model_name in c]\n",
    "    merged_df = merged_df.merge(df[cols_to_merge], on='id', how='inner')\n",
    "\n",
    "print(f'Merged: {len(merged_df)} samples with SGI from {len(model_names)} models')\n",
    "print(f'  Grounded: {merged_df[\"is_grounded\"].sum()}')\n",
    "print(f'  Hallucinated: {(~merged_df[\"is_grounded\"]).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross-Model Validation Tests\n",
    "\n",
    "### Test 1: Effect Size Consistency\n",
    "\n",
    "Does each embedding model show significant separation between grounded and hallucinated responses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('TEST 1: EFFECT SIZE CONSISTENCY')\n",
    "print('='*70)\n",
    "print(f'\\n{\"Model\":<10} | {\"Grounded\":>10} | {\"Halluc\":>10} | {\"Cohen\\'s d\":>10} | {\"AUROC\":>8} | {\"p-value\":>12}')\n",
    "print('-'*70)\n",
    "\n",
    "effect_sizes = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    sgi_col = f'sgi_{model_name}'\n",
    "    values = merged_df[sgi_col].values\n",
    "    labels = merged_df['is_grounded'].values\n",
    "    \n",
    "    result = compute_effect_size(values, labels, model_name)\n",
    "    effect_sizes[model_name] = result.to_dict()\n",
    "    \n",
    "    sig = '***' if result.p_value < 0.001 else '**' if result.p_value < 0.01 else '*' if result.p_value < 0.05 else ''\n",
    "    print(f'{model_name:<10} | {result.grounded_mean:>10.4f} | {result.hallucinated_mean:>10.4f} | '\n",
    "          f'{result.cohens_d:>+10.3f} | {result.auroc:>8.3f} | {result.p_value:>10.2e} {sig}')\n",
    "\n",
    "print('-'*70)\n",
    "\n",
    "# Summary\n",
    "d_values = [r['cohens_d'] for r in effect_sizes.values()]\n",
    "auc_values = [r['auroc'] for r in effect_sizes.values()]\n",
    "n_significant = sum(1 for r in effect_sizes.values() if r['significant'])\n",
    "\n",
    "print(f'\\nSignificant effects (p<0.05): {n_significant}/{len(model_names)}')\n",
    "print(f'Cohen\\'s d: mean={np.mean(d_values):.3f}, range=[{min(d_values):.3f}, {max(d_values):.3f}]')\n",
    "print(f'AUROC: mean={np.mean(auc_values):.3f}, range=[{min(auc_values):.3f}, {max(auc_values):.3f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Cross-Model Correlation (Pearson)\n",
    "\n",
    "If SGI captures something fundamental, scores from different models should correlate strongly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('TEST 2: CROSS-MODEL CORRELATION (PEARSON)')\n",
    "print('='*70)\n",
    "\n",
    "sgi_cols = [f'sgi_{m}' for m in model_names]\n",
    "sgi_df = merged_df[sgi_cols].copy()\n",
    "sgi_df.columns = model_names\n",
    "\n",
    "pearson_matrix = compute_correlation_matrix(sgi_df, model_names, method='pearson')\n",
    "pearson_stats = compute_pairwise_correlations(pearson_matrix)\n",
    "\n",
    "print('\\nPearson Correlation Matrix:')\n",
    "print(pearson_matrix.round(3).to_string())\n",
    "print(f'\\nOff-diagonal: mean={pearson_stats[\"mean\"]:.3f}, range=[{pearson_stats[\"min\"]:.3f}, {pearson_stats[\"max\"]:.3f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Ranking Agreement (Spearman)\n",
    "\n",
    "Do models agree on which samples are most/least grounded?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('TEST 3: RANKING AGREEMENT (SPEARMAN)')\n",
    "print('='*70)\n",
    "\n",
    "spearman_matrix = compute_correlation_matrix(sgi_df, model_names, method='spearman')\n",
    "spearman_stats = compute_pairwise_correlations(spearman_matrix)\n",
    "\n",
    "print('\\nSpearman Rank Correlation Matrix:')\n",
    "print(spearman_matrix.round(3).to_string())\n",
    "print(f'\\nOff-diagonal: mean={spearman_stats[\"mean\"]:.3f}, range=[{spearman_stats[\"min\"]:.3f}, {spearman_stats[\"max\"]:.3f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4: Top-K Flagging Agreement\n",
    "\n",
    "If we flag the top 10% as \"high risk\" by each model, how much overlap is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('TEST 4: TOP-K FLAGGING AGREEMENT')\n",
    "print('='*70)\n",
    "\n",
    "for k_pct in [10, 20, 30]:\n",
    "    overlap_matrix = compute_topk_overlap_matrix(merged_df, sgi_cols, k_percent=k_pct)\n",
    "    overlap_matrix.index = model_names\n",
    "    overlap_matrix.columns = model_names\n",
    "    \n",
    "    # Mean off-diagonal Jaccard\n",
    "    off_diag = [overlap_matrix.iloc[i, j] for i in range(len(model_names)) \n",
    "                for j in range(i+1, len(model_names))]\n",
    "    \n",
    "    print(f'\\nTop-{k_pct}% overlap (Jaccard similarity):')\n",
    "    print(overlap_matrix.round(3).to_string())\n",
    "    print(f'Mean pairwise Jaccard: {np.mean(off_diag):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5: Component Analysis\n",
    "\n",
    "Which component of SGI drives the signal — response-question proximity or response-context distance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('TEST 5: COMPONENT ANALYSIS')\n",
    "print('='*70)\n",
    "print(f'\\n{\"Model\":<10} | {\"d(θ_rq)\":>10} | {\"d(θ_rc)\":>10} | Driver')\n",
    "print('-'*50)\n",
    "\n",
    "for model_name in model_names:\n",
    "    theta_rq_col = f'theta_rq_{model_name}'\n",
    "    theta_rc_col = f'theta_rc_{model_name}'\n",
    "    labels = merged_df['is_grounded'].values\n",
    "    \n",
    "    d_rq = compute_effect_size(merged_df[theta_rq_col].values, labels, 'θ_rq')\n",
    "    d_rc = compute_effect_size(merged_df[theta_rc_col].values, labels, 'θ_rc')\n",
    "    \n",
    "    driver = 'θ(r,q)' if abs(d_rq.cohens_d) > abs(d_rc.cohens_d) else 'θ(r,c)'\n",
    "    print(f'{model_name:<10} | {d_rq.cohens_d:>+10.3f} | {d_rc.cohens_d:>+10.3f} | {driver}')\n",
    "\n",
    "print('-'*50)\n",
    "print('\\nInterpretation:')\n",
    "print('  • Negative d(θ_rq): Hallucinations are CLOSER to question')\n",
    "print('  • Positive d(θ_rc): Hallucinations are FARTHER from context')\n",
    "print('  • Both effects together = \"semantic laziness\" signature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = create_summary_figure(\n",
    "    df=merged_df,\n",
    "    effect_sizes=effect_sizes,\n",
    "    pearson_matrix=pearson_matrix,\n",
    "    spearman_matrix=spearman_matrix,\n",
    "    model_names=model_names,\n",
    "    save_path='cross_model_validation.png',\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgi.analysis import summarize_cross_model_validation\n",
    "\n",
    "summary = summarize_cross_model_validation(\n",
    "    effect_sizes={m: type('obj', (object,), effect_sizes[m])() for m in model_names},\n",
    "    pearson_corr=pearson_stats,\n",
    "    spearman_corr=spearman_stats,\n",
    ")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw results\n",
    "merged_df.to_csv('cross_model_sgi_results.csv', index=False)\n",
    "print('Results saved to cross_model_sgi_results.csv')\n",
    "\n",
    "# Summary statistics\n",
    "summary_stats = {\n",
    "    'n_samples': len(merged_df),\n",
    "    'n_grounded': int(merged_df['is_grounded'].sum()),\n",
    "    'n_hallucinated': int((~merged_df['is_grounded']).sum()),\n",
    "    'models': model_names,\n",
    "    'effect_sizes': effect_sizes,\n",
    "    'pearson_correlation': pearson_stats,\n",
    "    'spearman_correlation': spearman_stats,\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('cross_model_summary.json', 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "print('Summary saved to cross_model_summary.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}